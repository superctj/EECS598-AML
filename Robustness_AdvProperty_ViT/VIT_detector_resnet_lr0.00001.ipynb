{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VIT_detector_resnet——0428.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLswqBUeSU77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50da5ac9-bd0a-45d1-c352-d8ee610ade6f"
      },
      "source": [
        "#@title Installation\n",
        "!pip install timm foolbox"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting timm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/89/d94f59780b5dd973154bf506d8ce598f6bfe7cc44dd445d644d6d3be8c39/timm-0.4.5-py3-none-any.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 5.1MB/s \n",
            "\u001b[?25hCollecting foolbox\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ff/1ba817ad9aa7c2ca28fb809d64939bee7311e3e62fdd87c4011232c4640e/foolbox-3.3.1-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.1+cu101)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.8.1+cu101)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from foolbox) (1.4.1)\n",
            "Collecting requests>=2.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.7/dist-packages (from foolbox) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from foolbox) (1.19.5)\n",
            "Collecting eagerpy==0.29.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/07/54994565da4fc5a4840d3a434fb9bf3835b4a4e68c931ccfcc327d568f95/eagerpy-0.29.0-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from foolbox) (56.0.0)\n",
            "Collecting GitPython>=3.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 21.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.4MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: timm, requests, eagerpy, smmap, gitdb, GitPython, foolbox\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed GitPython-3.1.14 eagerpy-0.29.0 foolbox-3.3.1 gitdb-4.0.7 requests-2.25.1 smmap-4.0.0 timm-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhnaQeewS_rY"
      },
      "source": [
        "#@title Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWDxJaQ_TFxY"
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "import eagerpy as ep\n",
        "from foolbox import PyTorchModel, accuracy, samples\n",
        "from foolbox.attacks import L2PGD\n",
        "\n",
        "import timm\n",
        "\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP2MDluNTfPF"
      },
      "source": [
        "#@title Global Variables\n",
        "\n",
        "# data_dir = \"/content/drive/MyDrive/598dataset/Detector/\"\n",
        "# data_dir_fgsm = \"/content/drive/MyDrive/598dataset/Detector_fgsm/\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 32\n",
        "\n",
        "input_size = 224\n",
        "\n",
        "feature_extract = False\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF31WmOsTx1Y"
      },
      "source": [
        "#@title Utils\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('')\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            with tqdm(dataloaders[phase], unit='batch') as tepoch:\n",
        "                if phase == 'train':\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, labels in tepoch:\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        # Get model outputs and calculate loss\n",
        "                        # Special case for inception because in training it has an auxiliary output. In train\n",
        "                        #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                        #   but in testing we only consider the final output.\n",
        "                        if is_inception and phase == 'train':\n",
        "                            # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                            outputs, aux_outputs = model(inputs)\n",
        "                            loss1 = criterion(outputs, labels)\n",
        "                            loss2 = criterion(aux_outputs, labels)\n",
        "                            loss = loss1 + 0.4*loss2\n",
        "                        else:\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, labels)\n",
        "\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "                epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                if phase == 'val':\n",
        "                    val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history\n",
        "\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "class fft(object):\n",
        "  def __call__(self, sample):\n",
        "    sample = torch.fft.fft2(sample)\n",
        "    sample = torch.fft.fftshift(sample,dim=(-2,-1))\n",
        "    return sample.float()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCGm4O7oVjFQ"
      },
      "source": [
        "#@title Model\n",
        "model = models.resnet18(pretrained=False)\n",
        "num_ftr = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftr, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tFOmiF_Wlx3"
      },
      "source": [
        "#@title Dataloader Init\n",
        "data_dir = \"/content/drive/MyDrive/598dataset/detector_pgd_5/\"\n",
        "\n",
        "transform = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        fft()\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        fft()\n",
        "    ]),\n",
        "}\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          transform[x])\n",
        "                  for x in ['train', 'val']}\n",
        "                  \n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=0, pin_memory=True)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu9aDFUQByqA",
        "outputId": "13decb99-8e91-4718-808f-83f1e8a308b3"
      },
      "source": [
        "#@title resnet18 fft\n",
        "model = model.to(device)\n",
        "\n",
        "params_to_update = model.parameters()\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "optimizer = optim.Adam(params_to_update, lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model_ft, hist = train_model(model, dataloaders, criterion, optimizer, num_epochs=num_epochs, is_inception=False)\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/598dataset/Detector0428_ResNet50_lr0.0001.pth')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/516 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t conv1.weight\n",
            "\t bn1.weight\n",
            "\t bn1.bias\n",
            "\t layer1.0.conv1.weight\n",
            "\t layer1.0.bn1.weight\n",
            "\t layer1.0.bn1.bias\n",
            "\t layer1.0.conv2.weight\n",
            "\t layer1.0.bn2.weight\n",
            "\t layer1.0.bn2.bias\n",
            "\t layer1.1.conv1.weight\n",
            "\t layer1.1.bn1.weight\n",
            "\t layer1.1.bn1.bias\n",
            "\t layer1.1.conv2.weight\n",
            "\t layer1.1.bn2.weight\n",
            "\t layer1.1.bn2.bias\n",
            "\t layer2.0.conv1.weight\n",
            "\t layer2.0.bn1.weight\n",
            "\t layer2.0.bn1.bias\n",
            "\t layer2.0.conv2.weight\n",
            "\t layer2.0.bn2.weight\n",
            "\t layer2.0.bn2.bias\n",
            "\t layer2.0.downsample.0.weight\n",
            "\t layer2.0.downsample.1.weight\n",
            "\t layer2.0.downsample.1.bias\n",
            "\t layer2.1.conv1.weight\n",
            "\t layer2.1.bn1.weight\n",
            "\t layer2.1.bn1.bias\n",
            "\t layer2.1.conv2.weight\n",
            "\t layer2.1.bn2.weight\n",
            "\t layer2.1.bn2.bias\n",
            "\t layer3.0.conv1.weight\n",
            "\t layer3.0.bn1.weight\n",
            "\t layer3.0.bn1.bias\n",
            "\t layer3.0.conv2.weight\n",
            "\t layer3.0.bn2.weight\n",
            "\t layer3.0.bn2.bias\n",
            "\t layer3.0.downsample.0.weight\n",
            "\t layer3.0.downsample.1.weight\n",
            "\t layer3.0.downsample.1.bias\n",
            "\t layer3.1.conv1.weight\n",
            "\t layer3.1.bn1.weight\n",
            "\t layer3.1.bn1.bias\n",
            "\t layer3.1.conv2.weight\n",
            "\t layer3.1.bn2.weight\n",
            "\t layer3.1.bn2.bias\n",
            "\t layer4.0.conv1.weight\n",
            "\t layer4.0.bn1.weight\n",
            "\t layer4.0.bn1.bias\n",
            "\t layer4.0.conv2.weight\n",
            "\t layer4.0.bn2.weight\n",
            "\t layer4.0.bn2.bias\n",
            "\t layer4.0.downsample.0.weight\n",
            "\t layer4.0.downsample.1.weight\n",
            "\t layer4.0.downsample.1.bias\n",
            "\t layer4.1.conv1.weight\n",
            "\t layer4.1.bn1.weight\n",
            "\t layer4.1.bn1.bias\n",
            "\t layer4.1.conv2.weight\n",
            "\t layer4.1.bn2.weight\n",
            "\t layer4.1.bn2.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n",
            "\n",
            "Epoch 0/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:94: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /pytorch/aten/src/ATen/native/Copy.cpp:219.)\n",
            "100%|██████████| 516/516 [1:32:25<00:00, 10.75s/batch]\n",
            "  0%|          | 0/52 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.1311 Acc: 0.9682\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 52/52 [08:56<00:00, 10.31s/batch]\n",
            "  0%|          | 0/516 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 0.1787 Acc: 0.9697\n",
            "\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 516/516 [01:39<00:00,  5.16batch/s]\n",
            "  2%|▏         | 1/52 [00:00<00:07,  6.67batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.1091 Acc: 0.9699\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 52/52 [00:07<00:00,  6.69batch/s]\n",
            "  0%|          | 1/516 [00:00<01:39,  5.19batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 0.2220 Acc: 0.9667\n",
            "\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 516/516 [01:39<00:00,  5.19batch/s]\n",
            "  2%|▏         | 1/52 [00:00<00:07,  6.68batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.0802 Acc: 0.9724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 52/52 [00:07<00:00,  6.61batch/s]\n",
            "  0%|          | 1/516 [00:00<01:38,  5.25batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 0.2872 Acc: 0.9697\n",
            "\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 516/516 [01:39<00:00,  5.18batch/s]\n",
            "  2%|▏         | 1/52 [00:00<00:07,  6.85batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.0220 Acc: 0.9932\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 52/52 [00:08<00:00,  6.49batch/s]\n",
            "  0%|          | 0/516 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 0.1057 Acc: 0.9697\n",
            "\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 516/516 [01:39<00:00,  5.17batch/s]\n",
            "  2%|▏         | 1/52 [00:00<00:08,  6.34batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.0049 Acc: 0.9984\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 52/52 [00:07<00:00,  6.64batch/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 0.0313 Acc: 0.9921\n",
            "\n",
            "Training complete in 108m 32s\n",
            "Best val Acc: 0.992121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYam-DSksxGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5768ee2-2545-4ce5-bd15-8938199758ac"
      },
      "source": [
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/598dataset/ViTbase_16_224_cifar10.pth'))\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "batch_size = 32\n",
        "input_size = 224\n",
        "feature_extract = False\n",
        "num_epochs = 5\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_4DgCk7Td_O",
        "outputId": "70264a07-5f2d-4d03-a52a-ae8967040672"
      },
      "source": [
        "#@title test FGSM-L2 Epsilon=0.1\n",
        "\n",
        "class fft(object):\n",
        "  def __call__(self, sample):\n",
        "    sample = torch.fft.fft2(sample)\n",
        "    sample = torch.fft.fftshift(sample,dim=(-2,-1))\n",
        "    return sample.float()\n",
        "\n",
        "#@title Dataloader Init\n",
        "transform = {\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        fft()\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/598dataset/TinyImgNet_AdvSamples/VIT/FGSM/Epsilon0.1'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          transform[x])\n",
        "                  for x in ['val']}\n",
        "                  \n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,\n",
        "                                             shuffle=True, num_workers=4, pin_memory=True)\n",
        "              for x in ['val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['val']}\n",
        "class_names = image_datasets['val'].classes\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# get some constants\n",
        "NUM_CLASSES = len(class_names)\n",
        "\n",
        "counter = 0\n",
        "correct = 0\n",
        "with tqdm(dataloaders['val'], unit='batch') as tepoch:\n",
        "  for inputs, labels in tepoch:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    pred = model(inputs).argmax(axis=1)\n",
        "    # print(pred.shape, inputs.shape)\n",
        "    # print(pred)\n",
        "    correct +=  pred.shape[0] - pred.sum()\n",
        "    counter += pred.shape[0]\n",
        "\n",
        "print(f\"clean accuracy:  {correct/counter * 100:.1f} %\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [02:52<00:00,  3.44s/batch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "clean accuracy:  0.0 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV46mDsOYXyt",
        "outputId": "43f47fba-5cc2-467d-c25c-7409fb3eb16d"
      },
      "source": [
        "#@title test FGSM-L2 Epsilon=0.01\n",
        "\n",
        "class fft(object):\n",
        "  def __call__(self, sample):\n",
        "    sample = torch.fft.fft2(sample)\n",
        "    sample = torch.fft.fftshift(sample,dim=(-2,-1))\n",
        "    return sample.float()\n",
        "\n",
        "#@title Dataloader Init\n",
        "transform = {\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        fft()\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/598dataset/TinyImgNet_AdvSamples/VIT/FGSM/Epsilon0.01'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          transform[x])\n",
        "                  for x in ['val']}\n",
        "                  \n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,\n",
        "                                             shuffle=True, num_workers=4, pin_memory=True)\n",
        "              for x in ['val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['val']}\n",
        "class_names = image_datasets['val'].classes\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# get some constants\n",
        "NUM_CLASSES = len(class_names)\n",
        "\n",
        "counter = 0\n",
        "correct = 0\n",
        "with tqdm(dataloaders['val'], unit='batch') as tepoch:\n",
        "  for inputs, labels in tepoch:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    pred = model(inputs).argmax(axis=1)\n",
        "    # print(pred.shape, inputs.shape)\n",
        "    # print(pred)\n",
        "    correct +=  pred.shape[0] - pred.sum()\n",
        "    counter += pred.shape[0]\n",
        "\n",
        "print(f\"clean accuracy:  {correct/counter * 100:.1f} %\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [02:45<00:00,  3.30s/batch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "clean accuracy:  1.2 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}