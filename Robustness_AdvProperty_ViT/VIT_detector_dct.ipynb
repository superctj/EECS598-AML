{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VIT_detector_dct.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLswqBUeSU77",
        "outputId": "ac89ad86-6720-4096-e7ba-add6e5adb563"
      },
      "source": [
        "#@title Installation\n",
        "!pip install timm foolbox"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting timm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/89/d94f59780b5dd973154bf506d8ce598f6bfe7cc44dd445d644d6d3be8c39/timm-0.4.5-py3-none-any.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 8.4MB/s \n",
            "\u001b[?25hCollecting foolbox\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ff/1ba817ad9aa7c2ca28fb809d64939bee7311e3e62fdd87c4011232c4640e/foolbox-3.3.1-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 14.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.1+cu101)\n",
            "Collecting eagerpy==0.29.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/07/54994565da4fc5a4840d3a434fb9bf3835b4a4e68c931ccfcc327d568f95/eagerpy-0.29.0-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from foolbox) (56.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from foolbox) (1.19.5)\n",
            "Collecting GitPython>=3.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 35.9MB/s \n",
            "\u001b[?25hCollecting requests>=2.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from foolbox) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.7/dist-packages (from foolbox) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox) (1.24.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: timm, eagerpy, smmap, gitdb, GitPython, requests, foolbox\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed GitPython-3.1.14 eagerpy-0.29.0 foolbox-3.3.1 gitdb-4.0.7 requests-2.25.1 smmap-4.0.0 timm-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhnaQeewS_rY",
        "outputId": "9722355f-fe64-4105-c58f-7118764a55ff"
      },
      "source": [
        "#@title Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWDxJaQ_TFxY",
        "outputId": "9ebe969d-83c0-4dee-9ca7-ce66cec29a0d"
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "import eagerpy as ep\n",
        "from foolbox import PyTorchModel, accuracy, samples\n",
        "from foolbox.attacks import L2PGD\n",
        "\n",
        "import timm\n",
        "\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Version:  1.8.1+cu101\n",
            "Torchvision Version:  0.9.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP2MDluNTfPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992ac060-bcf7-418e-b93a-5a9b4bbf1b7b"
      },
      "source": [
        "#@title Global Variables\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 32\n",
        "\n",
        "input_size = 224\n",
        "\n",
        "feature_extract = False\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF31WmOsTx1Y"
      },
      "source": [
        "#@title Utils\n",
        "import scipy\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('')\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            with tqdm(dataloaders[phase], unit='batch') as tepoch:\n",
        "                if phase == 'train':\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, labels in tepoch:\n",
        "\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    \n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        # Get model outputs and calculate loss\n",
        "                        # Special case for inception because in training it has an auxiliary output. In train\n",
        "                        #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                        #   but in testing we only consider the final output.\n",
        "                        if is_inception and phase == 'train':\n",
        "                            # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                            outputs, aux_outputs = model(inputs)\n",
        "                            loss1 = criterion(outputs, labels)\n",
        "                            loss2 = criterion(aux_outputs, labels)\n",
        "                            loss = loss1 + 0.4*loss2\n",
        "                        else:\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, labels)\n",
        "\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "                epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                if phase == 'val':\n",
        "                    val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history\n",
        "\n",
        "def log_scale(array, epsilon=1e-12):\n",
        "    \"\"\"Log scale the input array.\n",
        "    \"\"\"\n",
        "    array = np.abs(array)\n",
        "    array += epsilon  # no zero in log\n",
        "    array = np.log(array)\n",
        "    return array\n",
        "\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "class dct(object):\n",
        "  def __call__(self, sample):\n",
        "    sample = sample.numpy()\n",
        "    sample = scipy.fft.dct(sample, type=2, norm =\"ortho\", axis = 1)\n",
        "    sample = scipy.fft.dct(sample, type=2, norm =\"ortho\", axis = 2)\n",
        "    sample = log_scale(sample)\n",
        "    return torch.from_numpy(sample)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylz19bjZ0v-J"
      },
      "source": [
        "import tensorflow as tf\n",
        "from PIL import ImageStat\n",
        "\n",
        "class Stats(ImageStat.Stat):\n",
        "  def __add__(self, other):\n",
        "    # add self.h and other.h element-wise\n",
        "    return Stats(list(np.add(self.h, other.h)))\n",
        "\n",
        "def fft_normalization(dataloader): \n",
        "  statistics = None   \n",
        "  toPIL=transforms.ToPILImage()\n",
        "  with tqdm(dataloader, unit='batch') as tepoch:\n",
        "    for data, _ in tepoch:\n",
        "        for b in range(data.shape[0]):\n",
        "            if statistics is None:\n",
        "                statistics = Stats(toPIL(data[b]))\n",
        "            else:\n",
        "                statistics += Stats(toPIL(data[b]))\n",
        "              \n",
        "  print(f'mean:{statistics.mean}, std:{statistics.stddev}')\n",
        "  return statistics.mean, statistics.stddev\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCGm4O7oVjFQ"
      },
      "source": [
        "#@title Model\n",
        "model = models.resnet18(pretrained=False)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tFOmiF_Wlx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78df02ac-3db5-4eaf-de15-1e4733dca037"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/598dataset/detector_pgd_5'\n",
        "# data_dir = \"/content/drive/MyDrive/598dataset/Detector/ResNet/pgd_2\"\n",
        "# data_dir_pgd = \"/content/drive/MyDrive/598dataset/Detector/ResNet/PGDL2_0.5\"\n",
        "\n",
        "#@title Dataloader Init\n",
        "transform = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        dct()\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        dct()\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          transform[x])\n",
        "                  for x in ['train', 'val']}\n",
        "                  \n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=1, pin_memory=True)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "\n",
        "fft_normalization(dataloaders['train'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 516/516 [1:19:50<00:00,  9.28s/batch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mean:[127.35689843025278, 127.32537054175363, 127.32858003101809], std:[74.05935663161335, 74.04663458858886, 74.0499986449985]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([127.35689843025278, 127.32537054175363, 127.32858003101809],\n",
              " [74.05935663161335, 74.04663458858886, 74.0499986449985])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv2Vi03R8_0z"
      },
      "source": [
        "#@title dataloader calib\n",
        "transform = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        dct(),\n",
        "        transforms.Normalize([127.35689843025278, 127.32537054175363, 127.32858003101809], [74.05935663161335, 74.04663458858886, 74.0499986449985])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        dct(),\n",
        "        transforms.Normalize([127.35689843025278, 127.32537054175363, 127.32858003101809], [74.05935663161335, 74.04663458858886, 74.0499986449985])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          transform[x])\n",
        "                  for x in ['train', 'val']}\n",
        "                  \n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=1, pin_memory=True)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu9aDFUQByqA",
        "outputId": "acd06f78-8bfc-4a04-b7f4-002d07e46f55"
      },
      "source": [
        "#@title resnet18 fft\n",
        "model = model.to(device)\n",
        "\n",
        "params_to_update = model.parameters()\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "optimizer = optim.Adam(params_to_update, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model_ft, hist = train_model(model, dataloaders, criterion, optimizer, num_epochs=num_epochs, is_inception=False)\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/598dataset/Detector/raw_norm_dct_norm_pgd5.pth')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/516 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t conv1.weight\n",
            "\t bn1.weight\n",
            "\t bn1.bias\n",
            "\t layer1.0.conv1.weight\n",
            "\t layer1.0.bn1.weight\n",
            "\t layer1.0.bn1.bias\n",
            "\t layer1.0.conv2.weight\n",
            "\t layer1.0.bn2.weight\n",
            "\t layer1.0.bn2.bias\n",
            "\t layer1.1.conv1.weight\n",
            "\t layer1.1.bn1.weight\n",
            "\t layer1.1.bn1.bias\n",
            "\t layer1.1.conv2.weight\n",
            "\t layer1.1.bn2.weight\n",
            "\t layer1.1.bn2.bias\n",
            "\t layer2.0.conv1.weight\n",
            "\t layer2.0.bn1.weight\n",
            "\t layer2.0.bn1.bias\n",
            "\t layer2.0.conv2.weight\n",
            "\t layer2.0.bn2.weight\n",
            "\t layer2.0.bn2.bias\n",
            "\t layer2.0.downsample.0.weight\n",
            "\t layer2.0.downsample.1.weight\n",
            "\t layer2.0.downsample.1.bias\n",
            "\t layer2.1.conv1.weight\n",
            "\t layer2.1.bn1.weight\n",
            "\t layer2.1.bn1.bias\n",
            "\t layer2.1.conv2.weight\n",
            "\t layer2.1.bn2.weight\n",
            "\t layer2.1.bn2.bias\n",
            "\t layer3.0.conv1.weight\n",
            "\t layer3.0.bn1.weight\n",
            "\t layer3.0.bn1.bias\n",
            "\t layer3.0.conv2.weight\n",
            "\t layer3.0.bn2.weight\n",
            "\t layer3.0.bn2.bias\n",
            "\t layer3.0.downsample.0.weight\n",
            "\t layer3.0.downsample.1.weight\n",
            "\t layer3.0.downsample.1.bias\n",
            "\t layer3.1.conv1.weight\n",
            "\t layer3.1.bn1.weight\n",
            "\t layer3.1.bn1.bias\n",
            "\t layer3.1.conv2.weight\n",
            "\t layer3.1.bn2.weight\n",
            "\t layer3.1.bn2.bias\n",
            "\t layer4.0.conv1.weight\n",
            "\t layer4.0.bn1.weight\n",
            "\t layer4.0.bn1.bias\n",
            "\t layer4.0.conv2.weight\n",
            "\t layer4.0.bn2.weight\n",
            "\t layer4.0.bn2.bias\n",
            "\t layer4.0.downsample.0.weight\n",
            "\t layer4.0.downsample.1.weight\n",
            "\t layer4.0.downsample.1.bias\n",
            "\t layer4.1.conv1.weight\n",
            "\t layer4.1.bn1.weight\n",
            "\t layer4.1.bn1.bias\n",
            "\t layer4.1.conv2.weight\n",
            "\t layer4.1.bn2.weight\n",
            "\t layer4.1.bn2.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n",
            "\n",
            "Epoch 0/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 516/516 [01:57<00:00,  4.41batch/s]\n",
            "  0%|          | 0/52 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.0186 Acc: 0.9950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 52/52 [07:29<00:00,  8.64s/batch]\n",
            "  0%|          | 0/516 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 0.3779 Acc: 0.9697\n",
            "\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 516/516 [01:56<00:00,  4.44batch/s]\n",
            "  0%|          | 0/52 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.0031 Acc: 0.9992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.63batch/s]\n",
            "  0%|          | 0/516 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 0.0402 Acc: 0.9927\n",
            "\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 516/516 [01:55<00:00,  4.46batch/s]\n",
            "  0%|          | 0/52 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.0027 Acc: 0.9995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.57batch/s]\n",
            "  0%|          | 0/516 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 0.0790 Acc: 0.9824\n",
            "\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 516/516 [01:55<00:00,  4.46batch/s]\n",
            "  0%|          | 0/52 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.0018 Acc: 0.9995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.58batch/s]\n",
            "  0%|          | 0/516 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 0.0364 Acc: 0.9927\n",
            "\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 516/516 [01:57<00:00,  4.39batch/s]\n",
            "  0%|          | 0/52 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.0006 Acc: 0.9998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.43batch/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 0.7610 Acc: 0.9697\n",
            "\n",
            "Training complete in 17m 58s\n",
            "Best val Acc: 0.992727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Km0N9lcJnqx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}