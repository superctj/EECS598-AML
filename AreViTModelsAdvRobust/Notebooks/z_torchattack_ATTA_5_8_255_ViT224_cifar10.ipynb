{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"z_torchattack_ATTA_5_8/255_ViT224_cifar10.ipynb","provenance":[{"file_id":"1XX5nol2FpsTSefwQgG1Ayv_UqqRwWgIb","timestamp":1618157737492}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"24a4ecd0ac954b6890abccc0d0a3b771":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b3be0a45eb094fa4ad0081c93fa805bc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_06cb45bd1c5a49eabd66bdc864d4aaba","IPY_MODEL_93f46701c30344d39fb4da93c1990429"]}},"b3be0a45eb094fa4ad0081c93fa805bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"06cb45bd1c5a49eabd66bdc864d4aaba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a4a2a924b9a44375a0b7b5b8e5390271","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":170498071,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":170498071,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7fa1a53ed0a74a4d9b004455cfbaec0f"}},"93f46701c30344d39fb4da93c1990429":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7b30b45f30064a31b9d3df3af6c5685a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170499072/? [00:04&lt;00:00, 38527590.51it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_78902e38d10e49a9a9270b6901a0b5d1"}},"a4a2a924b9a44375a0b7b5b8e5390271":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7fa1a53ed0a74a4d9b004455cfbaec0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b30b45f30064a31b9d3df3af6c5685a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"78902e38d10e49a9a9270b6901a0b5d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TJGSG89Uker4","executionInfo":{"status":"ok","timestamp":1618948119612,"user_tz":240,"elapsed":31098,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"ea36598b-4e53-4a00-ba47-124bb25a8071"},"source":["!git clone https://github.com/nateraw/huggingface-vit-finetune.git\n","%cd huggingface-vit-finetune/\n","!pip install -r requirements.txt\n","!pip install git+https://github.com/huggingface/transformers.git@master --upgrade\n","!pip install torchattacks"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'huggingface-vit-finetune'...\n","remote: Enumerating objects: 24, done.\u001b[K\n","remote: Counting objects: 100% (24/24), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 24 (delta 10), reused 19 (delta 6), pack-reused 0\u001b[K\n","Unpacking objects: 100% (24/24), done.\n","/content/huggingface-vit-finetune\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.8.1+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.9.1+cu101)\n","Collecting pytorch-lightning\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/99/68da5c6ca999de560036d98c492e507d17996f5eeb7e76ba64acd4bbb142/pytorch_lightning-1.2.8-py3-none-any.whl (841kB)\n","\u001b[K     |████████████████████████████████| 849kB 7.1MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (1.19.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 2)) (7.1.2)\n","Collecting fsspec[http]>=0.8.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n","\u001b[K     |████████████████████████████████| 112kB 25.6MB/s \n","\u001b[?25hCollecting PyYAML!=5.4.*,>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n","\u001b[K     |████████████████████████████████| 276kB 15.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r requirements.txt (line 3)) (4.41.1)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r requirements.txt (line 3)) (2.4.1)\n","Collecting torchmetrics>=0.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/79/e0e5bd38def3b53cf7abf7cf94c90b5d64248892f153c20a0c7d337b927c/torchmetrics-0.3.0-py3-none-any.whl (270kB)\n","\u001b[K     |████████████████████████████████| 276kB 25.4MB/s \n","\u001b[?25hCollecting future>=0.17.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n","\u001b[K     |████████████████████████████████| 829kB 26.3MB/s \n","\u001b[?25hRequirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 3)) (2.23.0)\n","Collecting aiohttp; extra == \"http\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 38.5MB/s \n","\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.28.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (54.2.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.15.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.12.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.32.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.12.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.0.1)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.36.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.3.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics>=0.2.0->pytorch-lightning->-r requirements.txt (line 3)) (20.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 3)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 3)) (2020.12.5)\n","Collecting async-timeout<4.0,>=3.0\n","  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n","Collecting multidict<7.0,>=4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n","\u001b[K     |████████████████████████████████| 143kB 52.9MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n","\u001b[K     |████████████████████████████████| 296kB 50.7MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 3)) (20.3.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.10.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics>=0.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.4.7)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.4.1)\n","Building wheels for collected packages: PyYAML, future\n","  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=1fdd20252bcd0385e65ecd1b3d2de655b52c4561d139b513135fd1f9b2833468\n","  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=418f8a84d90ed72ee858cdaec56f87d17028677fe6267e11bb23705d53db9674\n","  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n","Successfully built PyYAML future\n","Installing collected packages: async-timeout, multidict, yarl, aiohttp, fsspec, PyYAML, torchmetrics, future, pytorch-lightning\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","Successfully installed PyYAML-5.3.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.4.0 future-0.18.2 multidict-5.1.0 pytorch-lightning-1.2.8 torchmetrics-0.3.0 yarl-1.6.3\n","Collecting git+https://github.com/huggingface/transformers.git@master\n","  Cloning https://github.com/huggingface/transformers.git (to revision master) to /tmp/pip-req-build-fqsh1qui\n","  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-fqsh1qui\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n","Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.10.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 8.9MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n","Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n","Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n","Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n","Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 19.3MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n","Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n","Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n","Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2112546 sha256=654b639afabe4f59c8fbf385c1aecee0cff771c3d41d798a6daaa0120d70189e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-tnc8dgvh/wheels/03/01/00/f2c9020459e177a2729b25f0b9628d95bb967727d71a118170\n","Successfully built transformers\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.0.dev0\n","Collecting torchattacks\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/a8/559ba77713a870f3d288833455091753435daad414de5ccce64812199654/torchattacks-2.14.1-py3-none-any.whl (92kB)\n","\u001b[K     |████████████████████████████████| 102kB 5.8MB/s \n","\u001b[?25hInstalling collected packages: torchattacks\n","Successfully installed torchattacks-2.14.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c_fWRmZTkvTT","executionInfo":{"status":"ok","timestamp":1618948136956,"user_tz":240,"elapsed":4175,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from transformers import ViTFeatureExtractor, ViTForImageClassification\n","import requests\n","\n","import torchattacks\n","import time\n","import json"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zrXKbxgilRQd","executionInfo":{"status":"ok","timestamp":1618948147823,"user_tz":240,"elapsed":319,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"7d8478f5-7e1b-4e5e-a42d-89d0f345c8cb"},"source":["result_dict = {}\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ReR5Gilddzy","executionInfo":{"status":"ok","timestamp":1618948164922,"user_tz":240,"elapsed":15909,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"cb391855-1c3d-44b4-eef7-7aad29b0f78d"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136,"referenced_widgets":["24a4ecd0ac954b6890abccc0d0a3b771","b3be0a45eb094fa4ad0081c93fa805bc","06cb45bd1c5a49eabd66bdc864d4aaba","93f46701c30344d39fb4da93c1990429","a4a2a924b9a44375a0b7b5b8e5390271","7fa1a53ed0a74a4d9b004455cfbaec0f","7b30b45f30064a31b9d3df3af6c5685a","78902e38d10e49a9a9270b6901a0b5d1"]},"id":"XK6VtyXRlhCa","executionInfo":{"status":"ok","timestamp":1618948175313,"user_tz":240,"elapsed":8271,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"081dbeaa-1972-4762-c56e-9b4d4f57cae5"},"source":["transform = transforms.Compose(\n","    [\n","      transforms.Resize((224, 224)),\n","      transforms.ToTensor(),\n","      # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","cifar_trainloader = torch.utils.data.DataLoader(cifar_trainset, batch_size=32, shuffle=True, num_workers=2)\n","cifar_testloader = torch.utils.data.DataLoader(cifar_testset, batch_size=32, shuffle=False, num_workers=2)\n","cifar_classes = [\n","    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n","]"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24a4ecd0ac954b6890abccc0d0a3b771","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cJILmMrBoDg2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618948922683,"user_tz":240,"elapsed":3408,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"b1bdb3f3-0fb7-4ca6-e49c-b071f8e7cbff"},"source":["model = ViTForImageClassification.from_pretrained('nateraw/vit-base-patch16-224-cifar10', return_dict=False)\n","model.to(device)\n","# model = nn.Sequential(\n","#     nn.Upsample(scale_factor=7, mode='bilinear'),\n","#     model\n","# )\n"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ViTForImageClassification(\n","  (vit): ViTModel(\n","    (embeddings): ViTEmbeddings(\n","      (patch_embeddings): PatchEmbeddings(\n","        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","      )\n","      (dropout): Dropout(p=0.0, inplace=False)\n","    )\n","    (encoder): ViTEncoder(\n","      (layer): ModuleList(\n","        (0): ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (1): ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (2): ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (3): ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (4): ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (5): ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (6): ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (7): ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (8): ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (9): ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (10): ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (11): ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","  )\n","  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"8VpptLmtVj4X","executionInfo":{"status":"ok","timestamp":1618948208887,"user_tz":240,"elapsed":275,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}}},"source":["class Transformer(nn.Module):\n","  def __init__(self, model):\n","    super(Transformer, self).__init__()\n","    self.model = model\n","  def forward(self, x):\n","    return self.model(x)[0]\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"zS6B3t1MpVEJ","executionInfo":{"status":"ok","timestamp":1618948925965,"user_tz":240,"elapsed":252,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}}},"source":["ViT_model = Transformer(model).to(device)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJSiI8ms4hpx","executionInfo":{"status":"ok","timestamp":1618948906207,"user_tz":240,"elapsed":245,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}}},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(ViT_model.parameters(), lr=0.0002)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"nJ0zaBqHnDk1","executionInfo":{"status":"error","timestamp":1618948936442,"user_tz":240,"elapsed":6987,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"16c4d9cf-3537-4fec-fdcf-9579a986cd41"},"source":["# cifar-10 DATASET consists of 60000 images\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","  for i, (inputs, labels) in enumerate(cifar_testloader):\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    if(i % 10 == 0):\n","      print('Accuracy of the network on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy of the network on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"Transformer\"] = 100 * correct / total"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Accuracy of the network on 32 cifar-10 test images: 100.000000 %\n","Accuracy of the network on 352 cifar-10 test images: 99.431818 %\n","Accuracy of the network on 672 cifar-10 test images: 99.553571 %\n","Accuracy of the network on 992 cifar-10 test images: 99.596774 %\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-d97a03ac578f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy of the network on %d cifar-10 test images: %f %%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"I6FpJ63mb-Xu","executionInfo":{"status":"error","timestamp":1618948390713,"user_tz":240,"elapsed":5701,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"639d56a4-6fae-4423-cde2-67852e52bac1"},"source":["correct = 0\n","total = 0\n","with torch.no_grad():\n","  for i, (inputs, labels) in enumerate(cifar_trainloader):\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    if(i % 10 == 0):\n","      print('Accuracy of the network on %d cifar-10 train images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy of the network on %d cifar-10 train images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"Transformer\"] = 100 * correct / total"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Accuracy of the network on 32 cifar-10 train images: 100.000000 %\n","Accuracy of the network on 352 cifar-10 train images: 99.715909 %\n","Accuracy of the network on 672 cifar-10 train images: 99.851190 %\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-04bef7e41520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy of the network on %d cifar-10 train images: %f %%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":609},"id":"Rd5NEZ6iuKZy","executionInfo":{"status":"error","timestamp":1618949347716,"user_tz":240,"elapsed":407310,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"89faaed5-5616-4128-90e2-be2311a72b05"},"source":["# Train on clean train data\n","t0 = time.time()\n","\n","i = 0\n","correct = 0\n","total = 0\n","running_loss = 0.0\n","for (inputs, labels) in cifar_trainloader:\n","  # inputs & labels already on device\n","  # Adv training\n","  optimizer.zero_grad()\n","  inputs = inputs.to(device)\n","  labels = labels.to(device)\n","\n","  # forward + backward + optimize\n","  outputs = ViT_model(inputs)\n","  loss = criterion(outputs, labels)\n","  loss.backward()\n","  optimizer.step()\n","  running_loss += loss.item()\n","  \n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs.data, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  if i == 0:\n","    print(predicted, labels, \"\\n\", correct, total)\n","  # print(\"Here\", np.sum(predicted == labels))\n","  if i % 150 == 0:\n","    print('Training accuracy of ViT on %d clean train data: %f %%' % (total, 100 * correct / total))\n","    print(\"running loss: \", running_loss)\n","  i += 1\n","\n","print('Training accuracy on %d clean cifar-10 train images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ep1_train_acc\"] = 100 * correct / total\n","t1 = time.time()\n","result_dict[\"ViT_ep1_train_sec\"] = t1 - t0\n","result_dict"],"execution_count":29,"outputs":[{"output_type":"stream","text":["tensor([0, 8, 9, 5, 1, 5, 4, 0, 3, 9, 0, 2, 1, 2, 3, 3, 1, 9, 3, 0, 4, 8, 3, 2,\n","        3, 9, 7, 5, 2, 4, 3, 5], device='cuda:0') tensor([0, 8, 9, 5, 1, 5, 4, 0, 3, 9, 0, 2, 1, 2, 3, 3, 1, 9, 3, 0, 4, 8, 3, 2,\n","        3, 9, 7, 5, 2, 4, 3, 5], device='cuda:0') \n"," 32 32\n","Training accuracy of ViT on 32 clean train data: 100.000000 %\n","running loss:  0.005403601564466953\n","Training accuracy of ViT on 4832 clean train data: 100.000000 %\n","running loss:  0.8251057714223862\n","Training accuracy of ViT on 9632 clean train data: 99.989618 %\n","running loss:  1.6993686081841588\n","Training accuracy of ViT on 14432 clean train data: 99.986142 %\n","running loss:  2.764285245910287\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-00f7925b3dc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    group['eps'])\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"yFCXMB3kO0n-"},"source":["# ATTA Prepare Evaluation Adversarial Examples\n"]},{"cell_type":"code","metadata":{"id":"NoMHnfYo77HR"},"source":["atk_pgd_5 = torchattacks.PGD(ViT_model, eps=8/255, alpha=2/255, steps=5)\n","atk_pgd_10 = torchattacks.PGD(ViT_model, eps=8/255, alpha=2/255, steps=10)\n","atk_pgd_20 = torchattacks.PGD(ViT_model, eps=8/255, alpha=2/255, steps=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_HFDvefMJm-","executionInfo":{"status":"ok","timestamp":1618884585932,"user_tz":240,"elapsed":3523072,"user":{"displayName":"Zuoyi Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivqeYxDbE0Kvw3TT3-L9wa_k4tsto4Hkt1lQ0z=s64","userId":"15332262710819164300"}},"outputId":"d878dae8-cadf-4c03-f01c-0e0c81b0019a"},"source":["cifar_adv_test10 = []\n","start = time.time()\n","for i, (inputs, labels) in enumerate(cifar_testloader):\n","    adv_images = atk_pgd_10(inputs, labels)\n","    cifar_adv_test10.append((adv_images.cpu(), labels.cpu()))\n","    if i % 100 == 0:\n","      print(\"Progress(%): \", i*64/100)\n","\n","print(\"total time(sec) : %.2f\" % (time.time() - start))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Progress(%):  0.0\n","total time(sec) : 3522.79\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCRhgsk5Mi_U","executionInfo":{"status":"ok","timestamp":1618884760887,"user_tz":240,"elapsed":1235,"user":{"displayName":"Zuoyi Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivqeYxDbE0Kvw3TT3-L9wa_k4tsto4Hkt1lQ0z=s64","userId":"15332262710819164300"}},"outputId":"21b297d7-e394-4685-a612-9dd7e7711015"},"source":["# Store pgd 10 attacks\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd_10_adv_test.pt'\n","!touch $outfile_name\n","torch.save(cifar_adv_test10, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["pgd_10_adv_test.pt   pgd_5_adv_itr1_model.pt  README.gdoc\n","pgd_5_adv_itr1.json  pgd_5_adv_itr1.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vN9-RvK6DpPr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618928668479,"user_tz":240,"elapsed":3275,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"7113b344-d8c9-40aa-a461-bcfbe3679b89"},"source":["# Load pgd 10 attacks\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd_10_adv_test.pt'\n","cifar_adv_test10 = torch.load(outfile_name)\n","len(cifar_adv_test10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["157"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"ruBObKZHMc-M"},"source":["cifar_adv_test20 = []\n","start = time.time()\n","for i, (inputs, labels) in enumerate(cifar_testloader):\n","    adv_images = atk_pgd_20(inputs, labels)\n","    cifar_adv_test20.append((adv_images.cpu(), labels.cpu()))\n","    if i % 100 == 0:\n","      print(\"Progress(%): \", i*64/100)\n","\n","print(\"total time(sec) : %.2f\" % (time.time() - start))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WzL6N-o5MkO1"},"source":["# Store pgd 20 attacks\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd_20_adv_test.pt'\n","!touch $outfile_name\n","torch.save(cifar_adv_test20, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q9noMPXdDz2e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618928861562,"user_tz":240,"elapsed":3599,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"86416e1c-a6b4-4b0d-94a6-2a07af2660ca"},"source":["# Load pgd 20 attacks\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd_20_adv_test.pt'\n","cifar_adv_test20 = torch.load(outfile_name)\n","len(cifar_adv_test20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["157"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"lxzg8NbBpuDK"},"source":["# ATTA Training Epoch 1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kPmXSSwCZtvG","executionInfo":{"status":"ok","timestamp":1618943998973,"user_tz":240,"elapsed":2452560,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"c1db493b-ecec-455d-8f12-81fc084a14c9"},"source":["# Generate first itr adv images\n","cifar_adv = []\n","start = time.time()\n","for i, (inputs, labels) in enumerate(cifar_trainloader):\n","    adv_images = atk_pgd_5(inputs, labels)\n","    cifar_adv.append((adv_images.cpu(), labels.cpu()))\n","    if i % 100 == 0:\n","      print(\"Progress(%): \", 64*i/500)\n","\n","print(\"total time(sec) : %.2f\" % (time.time() - start))\n","\n","# Store\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd_5_adv_itr1_v2.pt'\n","!touch $outfile_name\n","torch.save(cifar_adv, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["total time(sec) : 2448.80\n","pgd_10_adv_test.pt\t pgd_5_adv_itr1_v2.pt\t  pgd_5_adv_itr4_model.pt\n","pgd_20_adv_test.pt\t pgd_5_adv_itr2_model.pt  pgd_5_adv_itr4.pt\n","pgd_5_adv_itr1.json\t pgd_5_adv_itr2.pt\t  pgd_5_adv_itr5_model.pt\n","pgd_5_adv_itr1_model.pt  pgd_5_adv_itr3_model.pt  pgd_5_adv_itr5.pt\n","pgd_5_adv_itr1.pt\t pgd_5_adv_itr3.pt\t  README.gdoc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XH8fpZHm1lp3","executionInfo":{"status":"ok","timestamp":1618889975653,"user_tz":240,"elapsed":6738,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"f39a2a8f-f045-4369-ae85-80041d3732e4"},"source":["# Load\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd_5_adv_itr1.pt'\n","cifar_adv = torch.load(outfile_name)\n","len(cifar_adv)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["782"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYXFJqGt55qR","executionInfo":{"status":"ok","timestamp":1618865613691,"user_tz":240,"elapsed":232,"user":{"displayName":"Zuoyi Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivqeYxDbE0Kvw3TT3-L9wa_k4tsto4Hkt1lQ0z=s64","userId":"15332262710819164300"}},"outputId":"5a6445c8-0558-4864-bdbb-fd366fa2fd67"},"source":["inputs, labels = cifar_adv[0]\n","labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 5, 1, 2, 2, 7, 4, 1, 9, 1, 2, 6, 3, 9, 9, 0, 8, 8, 2, 4, 3, 9, 7, 4,\n","        6, 0, 4, 8, 0, 1, 4, 7, 0, 3, 5, 6, 1, 5, 8, 7, 1, 8, 9, 8, 8, 5, 1, 9,\n","        2, 0, 5, 1, 2, 1, 6, 2, 5, 2, 9, 4, 8, 5, 4, 4])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QG1R8Npl9iOB","executionInfo":{"status":"ok","timestamp":1618866870689,"user_tz":240,"elapsed":1156111,"user":{"displayName":"Zuoyi Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivqeYxDbE0Kvw3TT3-L9wa_k4tsto4Hkt1lQ0z=s64","userId":"15332262710819164300"}},"outputId":"c01f0088-8ccf-472c-c6aa-7325fa9dafef"},"source":["t0 = time.time()\n","\n","i = 0\n","correct = 0\n","total = 0\n","running_loss = 0.0\n","for (inputs, labels) in cifar_adv:\n","  # inputs & labels already on device\n","  # Adv training\n","  optimizer.zero_grad()\n","  inputs = inputs.to(device)\n","  labels = labels.to(device)\n","\n","  # forward + backward + optimize\n","  outputs = ViT_model(inputs)\n","  loss = criterion(outputs, labels)\n","  loss.backward()\n","  optimizer.step()\n","  running_loss += loss.item()\n","  \n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs.data, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  # print(\"Here\", np.sum(predicted == labels))\n","  if i % 150 == 0:\n","    print('Training accuracy of ViT on %d adv exs: %f %%' % (total, 100 * correct / total))\n","    print(\"running loss: \", running_loss)\n","  i += 1\n","\n","print('Training accuracy on %d pgd-5 cifar-10 adv images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep1_train_acc\"] = 100 * correct / total\n","t1 = time.time()\n","result_dict[\"ViT_ATTA_5_ep1_train_sec\"] = t1 - t0\n","result_dict\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy of ViT on 64 adv exs: 7.812500 %\n","running loss:  7.340195178985596\n","Training accuracy of ViT on 9664 adv exs: 10.885762 %\n","running loss:  357.84424209594727\n","Training accuracy of ViT on 19264 adv exs: 11.238580 %\n","running loss:  704.4925017356873\n","Training accuracy of ViT on 28864 adv exs: 11.089939 %\n","running loss:  1050.6746814250946\n","Training accuracy of ViT on 38464 adv exs: 11.020695 %\n","running loss:  1396.699630498886\n","Training accuracy of ViT on 48064 adv exs: 10.925017 %\n","running loss:  1742.6470937728882\n","Training accuracy on 50000 pgd-5 cifar-10 adv images: 10.960000 %\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'ViT_ATTA_5_ep1_train_acc': 10.96,\n"," 'ViT_ATTA_5_ep1_train_sec': 1155.8513135910034}"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_-jJJNNmJQQ","executionInfo":{"status":"ok","timestamp":1618944903379,"user_tz":240,"elapsed":657346,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"42f6c7a5-bfb0-488e-98b6-3242b6de33d6"},"source":["# Training V2 log\n","t0 = time.time()\n","\n","i = 0\n","correct = 0\n","total = 0\n","running_loss = 0.0\n","for (inputs, labels) in cifar_adv:\n","  # inputs & labels already on device\n","  # Adv training\n","  optimizer.zero_grad()\n","  inputs = inputs.to(device)\n","  labels = labels.to(device)\n","\n","  # forward + backward + optimize\n","  outputs = ViT_model(inputs)\n","  loss = criterion(outputs, labels)\n","  loss.backward()\n","  optimizer.step()\n","  running_loss += loss.item()\n","  \n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs.data, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  # print(\"Here\", np.sum(predicted == labels))\n","  if i % 150 == 0:\n","    print('Training accuracy of ViT on %d adv exs: %f %%' % (total, 100 * correct / total))\n","    print(\"running loss: \", running_loss)\n","  i += 1\n","\n","print('Training accuracy on %d pgd-5 cifar-10 adv images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep1_train_acc\"] = 100 * correct / total\n","t1 = time.time()\n","result_dict[\"ViT_ATTA_5_ep1_train_sec\"] = t1 - t0\n","result_dict\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy of ViT on 64 adv exs: 15.625000 %\n","running loss:  7.470674991607666\n","Training accuracy of ViT on 9664 adv exs: 11.392798 %\n","running loss:  357.2774474620819\n","Training accuracy of ViT on 19264 adv exs: 11.290490 %\n","running loss:  703.5805416107178\n","Training accuracy of ViT on 28864 adv exs: 11.141907 %\n","running loss:  1049.879697561264\n","Training accuracy of ViT on 38464 adv exs: 10.947899 %\n","running loss:  1396.1210498809814\n","Training accuracy of ViT on 48064 adv exs: 10.956225 %\n","running loss:  1742.033133983612\n","Training accuracy on 50000 pgd-5 cifar-10 adv images: 10.938000 %\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'ATTA_ep5_Orig_Acc': 10.0,\n"," 'Transformer': 98.74,\n"," 'ViT_ATTA_5_ep1_train_acc': 10.938,\n"," 'ViT_ATTA_5_ep1_train_sec': 656.953905582428}"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XNDz7GDhAOZG","executionInfo":{"status":"ok","timestamp":1618944950494,"user_tz":240,"elapsed":1800,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"d2ca7654-1ed4-41e6-dc80-b579ac67c4e1"},"source":["# Store model after itr1\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd_5_adv_itr1_v2_model.pt'\n","!touch $outfile_name\n","torch.save(ViT_model, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["pgd_10_adv_test.pt\t    pgd_5_adv_itr1_v2.pt     pgd_5_adv_itr4.pt\n","pgd_20_adv_test.pt\t    pgd_5_adv_itr2_model.pt  pgd_5_adv_itr5_model.pt\n","pgd_5_adv_itr1.json\t    pgd_5_adv_itr2.pt\t     pgd_5_adv_itr5.pt\n","pgd_5_adv_itr1_model.pt     pgd_5_adv_itr3_model.pt  README.gdoc\n","pgd_5_adv_itr1.pt\t    pgd_5_adv_itr3.pt\n","pgd_5_adv_itr1_v2_model.pt  pgd_5_adv_itr4_model.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WIsNSsyoB90y"},"source":["# Load model\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd_5_adv_itr1_model.pt'\n","ViT_model = torch.load(outfile_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BnnuCByzqILw"},"source":["# Evaluate ATTA Epoch 1 Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DOsSqgaB066g","executionInfo":{"status":"ok","timestamp":1618885684579,"user_tz":240,"elapsed":123210,"user":{"displayName":"Zuoyi Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivqeYxDbE0Kvw3TT3-L9wa_k4tsto4Hkt1lQ0z=s64","userId":"15332262710819164300"}},"outputId":"e8068330-a7a6-44de-c030-4fb00cd3075b"},"source":["# Evaluate model on pgd-10 test adv\n","correct = 0\n","total = 0\n","for i, (inputs, labels) in enumerate(cifar_adv_test10):\n","  inputs, labels = inputs.to(device), labels.to(device)\n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  if(i % 100 == 0):\n","    print('Accuracy on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy ATTA5-ep1 ViT on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep1_test_pgd10_acc\"] = 100 * correct / total"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy on 64 cifar-10 test images: 4.687500 %\n","Accuracy on 6464 cifar-10 test images: 9.823639 %\n","Accuracy ATTA5-ep1 ViT on 10000 cifar-10 test images: 10.000000 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"myBc4RbQHzwc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889704092,"user_tz":240,"elapsed":105655,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"8ee94d1e-407c-4303-f502-c481abe82954"},"source":["# Evaluate model on pgd-20 test adv\n","correct = 0\n","total = 0\n","for i, (inputs, labels) in enumerate(cifar_adv_test20):\n","  inputs, labels = inputs.to(device), labels.to(device)\n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  if(i % 100 == 0):\n","    print('Accuracy on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy ATTA5-ep1 ViT on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep1_test_pgd20_acc\"] = 100 * correct / total"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy on 64 cifar-10 test images: 4.687500 %\n","Accuracy on 6464 cifar-10 test images: 9.823639 %\n","Accuracy ATTA5-ep1 ViT on 10000 cifar-10 test images: 10.000000 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3PXzH8sSpfux"},"source":["# ATTA Training Epoch 2\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kgck_3nWIQaj","executionInfo":{"status":"ok","timestamp":1618898617371,"user_tz":240,"elapsed":8496346,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"ad19aff0-eefe-4b15-bf22-19deeb48d6c9"},"source":["# ATTA generate attack ep2\n","i = 0\n","t0 = time.time()\n","for (inputs, labels) in cifar_adv:\n","  adv_images = atk_pgd_5(inputs, labels)\n","  cifar_adv[i] = (adv_images.cpu(), labels.cpu())\n","  if i % 100 == 0:\n","    print(\"Progress(%): \", 64*i/500)\n","  i += 1\n","\n","time_gen_atk_2 = time.time() - t0\n","print(\"time lap(sec):\", time_gen_atk_2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Progress(%):  0.0\n","Progress(%):  16.0\n","Progress(%):  32.0\n","Progress(%):  48.0\n","Progress(%):  64.0\n","Progress(%):  80.0\n","Progress(%):  96.0\n","Progress(%):  112.0\n","time lap(sec): 8496.179186105728\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cs9ZmK1RYBel","executionInfo":{"status":"ok","timestamp":1618898621211,"user_tz":240,"elapsed":2875,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"53036a5d-e478-4af1-f0f2-1078730799ac"},"source":["# Store\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd_5_adv_itr2.pt'\n","!touch $outfile_name\n","torch.save(cifar_adv, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["pgd_10_adv_test.pt  pgd_5_adv_itr1.json      pgd_5_adv_itr1.pt\tREADME.gdoc\n","pgd_20_adv_test.pt  pgd_5_adv_itr1_model.pt  pgd_5_adv_itr2.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pj43NI3OYE8S","executionInfo":{"status":"ok","timestamp":1618930031205,"user_tz":240,"elapsed":5802,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"7f9bffb6-ce6d-4ee6-d726-03a4653167bf"},"source":["# Load\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd_5_adv_itr2.pt'\n","cifar_adv = torch.load(outfile_name)\n","len(cifar_adv)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["782"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfgYXuejXzPF","executionInfo":{"status":"ok","timestamp":1618901004640,"user_tz":240,"elapsed":2272461,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"17ddc81c-5527-428e-ed38-5f645c58dd44"},"source":["# ATTA train ep2\n","t0 = time.time()\n","\n","i = 0\n","correct = 0\n","total = 0\n","running_loss = 0.0\n","for (inputs, labels) in cifar_adv:\n","  # inputs & labels already on device\n","  # Adv training\n","  optimizer.zero_grad()\n","  inputs = inputs.to(device)\n","  labels = labels.to(device)\n","\n","  # forward + backward + optimize\n","  outputs = ViT_model(inputs)\n","  loss = criterion(outputs, labels)\n","  loss.backward()\n","  optimizer.step()\n","  running_loss += loss.item()\n","  \n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs.data, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  # print(\"Here\", np.sum(predicted == labels))\n","  if i % 150 == 0:\n","    print('Training accuracy of ViT on %d adv exs: %f %%' % (total, 100 * correct / total))\n","    print(\"running loss: \", running_loss)\n","  i += 1\n","\n","print('Training accuracy on %d pgd-5 cifar-10 adv images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep2_train_acc\"] = 100 * correct / total\n","t1 = time.time()\n","result_dict[\"ViT_ATTA_5_ep2_train_sec\"] = t1 - t0\n","\n","# Store model after itr2\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd_5_adv_itr2_model.pt'\n","!touch $outfile_name\n","torch.save(ViT_model, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/\n","\n","result_dict"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy of ViT on 64 adv exs: 15.625000 %\n","running loss:  2.2928543090820312\n","Training accuracy of ViT on 9664 adv exs: 11.951573 %\n","running loss:  349.6787316799164\n","Training accuracy of ViT on 19264 adv exs: 12.998339 %\n","running loss:  691.1132817268372\n","Training accuracy of ViT on 28864 adv exs: 13.979351 %\n","running loss:  1028.301528453827\n","Training accuracy of ViT on 38464 adv exs: 13.753120 %\n","running loss:  1369.2223527431488\n","Training accuracy of ViT on 48064 adv exs: 15.446072 %\n","running loss:  1673.916489124298\n","Training accuracy on 50000 pgd-5 cifar-10 adv images: 15.862000 %\n","pgd_10_adv_test.pt   pgd_5_adv_itr1_model.pt  pgd_5_adv_itr2.pt\n","pgd_20_adv_test.pt   pgd_5_adv_itr1.pt\t      README.gdoc\n","pgd_5_adv_itr1.json  pgd_5_adv_itr2_model.pt\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'ViT_ATTA_5_ep1_test_pgd20_acc': 10.0,\n"," 'ViT_ATTA_5_ep2_train_acc': 15.862,\n"," 'ViT_ATTA_5_ep2_train_sec': 2270.394117832184}"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"GeYOrLd5p-qS"},"source":["# Load model\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd_5_adv_itr2_model.pt'\n","ViT_model = torch.load(outfile_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EKkOvomSqO_V"},"source":["# Evaluate ATTA Epoch 2 Model\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4pFyKFKUqUq3","executionInfo":{"status":"ok","timestamp":1618928748800,"user_tz":240,"elapsed":32132,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"26027272-2390-47d7-f869-ca5c8e662668"},"source":["# Evaluate model on pgd-10 test adv\n","correct = 0\n","total = 0\n","for i, (inputs, labels) in enumerate(cifar_adv_test10):\n","  inputs, labels = inputs.to(device), labels.to(device)\n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  if(i % 100 == 0):\n","    print('Accuracy on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy ATTA5-ep2 ViT on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep2_test_pgd10_acc\"] = 100 * correct / total"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy on 64 cifar-10 test images: 15.625000 %\n","Accuracy on 6464 cifar-10 test images: 13.474629 %\n","Accuracy ATTA5-ep2 ViT on 10000 cifar-10 test images: 13.450000 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qlqWgSj8qOKV","executionInfo":{"status":"ok","timestamp":1618928903819,"user_tz":240,"elapsed":32063,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"2ad367f7-df4c-417e-8190-e64768494c52"},"source":["# Evaluate model on pgd-20 test adv\n","correct = 0\n","total = 0\n","for i, (inputs, labels) in enumerate(cifar_adv_test20):\n","  inputs, labels = inputs.to(device), labels.to(device)\n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  if(i % 100 == 0):\n","    print('Accuracy on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy ATTA5-ep2 ViT on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep2_test_pgd20_acc\"] = 100 * correct / total"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy on 64 cifar-10 test images: 15.625000 %\n","Accuracy on 6464 cifar-10 test images: 13.180693 %\n","Accuracy ATTA5-ep2 ViT on 10000 cifar-10 test images: 13.120000 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KldxoayBsuYT"},"source":["# ATTA Training Epoch 3 ~\n"]},{"cell_type":"code","metadata":{"id":"7HRkvJFc-Myh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618939246453,"user_tz":240,"elapsed":9184806,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"7bc605c3-0459-474b-9b31-72720dfd9040"},"source":["output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","t0 = time.time()\n","for ep in range(3):\n","  print(\"##########    Epoch \" + str(ep+3) + \"    ##########\")\n","  ts = time.time()\n","  i = 0\n","  for (inputs, labels) in cifar_adv:\n","    adv_images = atk_pgd_5(inputs, labels)\n","    cifar_adv[i] = (adv_images.cpu(), labels.cpu())\n","    if i % 100 == 0:\n","      print(\"Progress(%): \", 64*i/500)\n","    i += 1\n","  time_gen_atk_2 = time.time() - ts\n","  print(\"time lap(sec):\", time_gen_atk_2)\n","  time_name = \"ViT_ATTA_5_ep\" + str(ep+3) + \"_gen_atk_sec\"\n","  result_dict[time_name] = time_gen_atk_2\n","\n","  # Store cifar-10\n","  outfile_name = 'pgd_5_adv_itr' + str(ep+3) + '.pt'\n","  outfile_name = output_dir + outfile_name\n","  !touch $outfile_name\n","  torch.save(cifar_adv, outfile_name)\n","  !ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/\n","\n","  ts = time.time()\n","  i = 0\n","  correct = 0\n","  total = 0\n","  running_loss = 0.0\n","  for (inputs, labels) in cifar_adv:\n","    # inputs & labels already on device\n","    # Adv training\n","    optimizer.zero_grad()\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","\n","    # forward + backward + optimize\n","    outputs = ViT_model(inputs)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    running_loss += loss.item()\n","    \n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    # print(\"Here\", np.sum(predicted == labels))\n","    if i % 150 == 0:\n","      print('Training accuracy of ViT on %d adv exs: %f %%' % (total, 100 * correct / total))\n","      print(\"running loss: \", running_loss)\n","    i += 1\n","\n","  print('Training accuracy on %d pgd-5 cifar-10 adv images: %f %%' % (\n","      total,\n","      100 * correct / total))\n","  t1 = time.time()\n","  acc_name = \"ViT_ATTA_5_ep\" + str(ep+3) + \"_train_acc\"\n","  time_name = \"ViT_ATTA_5_ep\" + str(ep+3) + \"_train_sec\"\n","  result_dict[acc_name] = 100 * correct / total\n","  result_dict[time_name] = t1 - ts\n","\n","  # Store model after itr ep+3\n","  outfile_name = 'pgd_5_adv_itr' + str(ep+3) + '_model.pt'\n","  outfile_name = output_dir + outfile_name\n","  !touch $outfile_name\n","  torch.save(ViT_model, outfile_name)\n","  !ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/\n","\n","t1 = time.time()\n","result_dict[\"ViT_3eps_ATTA_cifar10_sec\"] = t1 - t0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["##########    Epoch 3    ##########\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Progress(%):  0.0\n","Progress(%):  12.8\n","Progress(%):  25.6\n","Progress(%):  38.4\n","Progress(%):  51.2\n","Progress(%):  64.0\n","Progress(%):  76.8\n","Progress(%):  89.6\n","time lap(sec): 2409.723920583725\n","pgd_10_adv_test.pt   pgd_5_adv_itr1_model.pt  pgd_5_adv_itr2.pt\n","pgd_20_adv_test.pt   pgd_5_adv_itr1.pt\t      pgd_5_adv_itr3.pt\n","pgd_5_adv_itr1.json  pgd_5_adv_itr2_model.pt  README.gdoc\n","Training accuracy of ViT on 64 adv exs: 17.187500 %\n","running loss:  3.2737040519714355\n","Training accuracy of ViT on 9664 adv exs: 19.474338 %\n","running loss:  324.05403101444244\n","Training accuracy of ViT on 19264 adv exs: 21.797135 %\n","running loss:  622.9230260848999\n","Training accuracy of ViT on 28864 adv exs: 23.330100 %\n","running loss:  911.7267687320709\n","Training accuracy of ViT on 38464 adv exs: 24.295445 %\n","running loss:  1198.0771329402924\n","Training accuracy of ViT on 48064 adv exs: 22.790446 %\n","running loss:  1530.6852853298187\n","Training accuracy on 50000 pgd-5 cifar-10 adv images: 22.488000 %\n","pgd_10_adv_test.pt\t pgd_5_adv_itr1.pt\t  pgd_5_adv_itr3.pt\n","pgd_20_adv_test.pt\t pgd_5_adv_itr2_model.pt  README.gdoc\n","pgd_5_adv_itr1.json\t pgd_5_adv_itr2.pt\n","pgd_5_adv_itr1_model.pt  pgd_5_adv_itr3_model.pt\n","##########    Epoch 4    ##########\n","Progress(%):  0.0\n","Progress(%):  12.8\n","Progress(%):  25.6\n","Progress(%):  38.4\n","Progress(%):  51.2\n","Progress(%):  64.0\n","Progress(%):  76.8\n","Progress(%):  89.6\n","time lap(sec): 2411.9986884593964\n","pgd_10_adv_test.pt\t pgd_5_adv_itr1.pt\t  pgd_5_adv_itr3.pt\n","pgd_20_adv_test.pt\t pgd_5_adv_itr2_model.pt  pgd_5_adv_itr4.pt\n","pgd_5_adv_itr1.json\t pgd_5_adv_itr2.pt\t  README.gdoc\n","pgd_5_adv_itr1_model.pt  pgd_5_adv_itr3_model.pt\n","Training accuracy of ViT on 64 adv exs: 9.375000 %\n","running loss:  2.6308915615081787\n","Training accuracy of ViT on 9664 adv exs: 12.282699 %\n","running loss:  347.8926589488983\n","Training accuracy of ViT on 19264 adv exs: 13.486296 %\n","running loss:  683.7199301719666\n","Training accuracy of ViT on 28864 adv exs: 14.176829 %\n","running loss:  1014.9581837654114\n","Training accuracy of ViT on 38464 adv exs: 14.626664 %\n","running loss:  1344.0087369680405\n","Training accuracy of ViT on 48064 adv exs: 14.975866 %\n","running loss:  1672.8472200632095\n","Training accuracy on 50000 pgd-5 cifar-10 adv images: 15.032000 %\n","pgd_10_adv_test.pt\t pgd_5_adv_itr1.pt\t  pgd_5_adv_itr3.pt\n","pgd_20_adv_test.pt\t pgd_5_adv_itr2_model.pt  pgd_5_adv_itr4_model.pt\n","pgd_5_adv_itr1.json\t pgd_5_adv_itr2.pt\t  pgd_5_adv_itr4.pt\n","pgd_5_adv_itr1_model.pt  pgd_5_adv_itr3_model.pt  README.gdoc\n","##########    Epoch 5    ##########\n","Progress(%):  0.0\n","Progress(%):  12.8\n","Progress(%):  25.6\n","Progress(%):  38.4\n","Progress(%):  51.2\n","Progress(%):  64.0\n","Progress(%):  76.8\n","Progress(%):  89.6\n","time lap(sec): 2400.8721063137054\n","pgd_10_adv_test.pt\t pgd_5_adv_itr2_model.pt  pgd_5_adv_itr4.pt\n","pgd_20_adv_test.pt\t pgd_5_adv_itr2.pt\t  pgd_5_adv_itr5.pt\n","pgd_5_adv_itr1.json\t pgd_5_adv_itr3_model.pt  README.gdoc\n","pgd_5_adv_itr1_model.pt  pgd_5_adv_itr3.pt\n","pgd_5_adv_itr1.pt\t pgd_5_adv_itr4_model.pt\n","Training accuracy of ViT on 64 adv exs: 15.625000 %\n","running loss:  2.581387519836426\n","Training accuracy of ViT on 9664 adv exs: 14.279801 %\n","running loss:  346.08757972717285\n","Training accuracy of ViT on 19264 adv exs: 15.220100 %\n","running loss:  678.7258172035217\n","Training accuracy of ViT on 28864 adv exs: 16.071924 %\n","running loss:  1005.1083958148956\n","Training accuracy of ViT on 38464 adv exs: 16.724730 %\n","running loss:  1330.0258276462555\n","Training accuracy of ViT on 48064 adv exs: 17.815829 %\n","running loss:  1638.4444913864136\n","Training accuracy on 50000 pgd-5 cifar-10 adv images: 17.956000 %\n","pgd_10_adv_test.pt\t pgd_5_adv_itr2_model.pt  pgd_5_adv_itr4.pt\n","pgd_20_adv_test.pt\t pgd_5_adv_itr2.pt\t  pgd_5_adv_itr5_model.pt\n","pgd_5_adv_itr1.json\t pgd_5_adv_itr3_model.pt  pgd_5_adv_itr5.pt\n","pgd_5_adv_itr1_model.pt  pgd_5_adv_itr3.pt\t  README.gdoc\n","pgd_5_adv_itr1.pt\t pgd_5_adv_itr4_model.pt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lx7akgsEPbte"},"source":["# Evaluate ATTA Epoch 5 Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HDjXofocPhLD","executionInfo":{"status":"ok","timestamp":1618939332712,"user_tz":240,"elapsed":32197,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"3caa1c0f-6cf5-4219-c2de-eeb0870c8ed3"},"source":["# Evaluate model on pgd-10 test adv\n","correct = 0\n","total = 0\n","for i, (inputs, labels) in enumerate(cifar_adv_test10):\n","  inputs, labels = inputs.to(device), labels.to(device)\n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  if(i % 100 == 0):\n","    print('Accuracy on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy ATTA5-ep5 ViT on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep5_test_pgd10_acc\"] = 100 * correct / total"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy on 64 cifar-10 test images: 14.062500 %\n","Accuracy on 6464 cifar-10 test images: 14.681312 %\n","Accuracy ATTA5-ep5 ViT on 10000 cifar-10 test images: 14.680000 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77gkn_xbPoMz","executionInfo":{"status":"ok","timestamp":1618939383409,"user_tz":240,"elapsed":32227,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"826c26ef-dde2-4211-a233-49f44d919779"},"source":["# Evaluate model on pgd-20 test adv\n","correct = 0\n","total = 0\n","for i, (inputs, labels) in enumerate(cifar_adv_test20):\n","  inputs, labels = inputs.to(device), labels.to(device)\n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  if(i % 100 == 0):\n","    print('Accuracy on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy ATTA5-ep5 ViT on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep5_test_pgd20_acc\"] = 100 * correct / total"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy on 64 cifar-10 test images: 12.500000 %\n","Accuracy on 6464 cifar-10 test images: 14.418317 %\n","Accuracy ATTA5-ep5 ViT on 10000 cifar-10 test images: 14.310000 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9kAyW-eswECa"},"source":["# Evaluate Model on Original Data\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhF0N5CsZggK","executionInfo":{"status":"ok","timestamp":1618941035063,"user_tz":240,"elapsed":33045,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"8ef8006b-5bc8-42e1-ce77-1e5f98c256bb"},"source":["# Epoch 1 Model on Original test data\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","  for i, (inputs, labels) in enumerate(cifar_testloader):\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    if(i % 10 == 0):\n","      print('Accuracy of the network on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy of the network on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ATTA_ep5_Orig_Acc\"] = 100 * correct / total"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy of the network on 64 cifar-10 test images: 4.687500 %\n","Accuracy of the network on 704 cifar-10 test images: 7.954545 %\n","Accuracy of the network on 1344 cifar-10 test images: 9.672619 %\n","Accuracy of the network on 1984 cifar-10 test images: 9.929435 %\n","Accuracy of the network on 2624 cifar-10 test images: 9.603659 %\n","Accuracy of the network on 3264 cifar-10 test images: 9.681373 %\n","Accuracy of the network on 3904 cifar-10 test images: 9.759221 %\n","Accuracy of the network on 4544 cifar-10 test images: 9.925176 %\n","Accuracy of the network on 5184 cifar-10 test images: 10.069444 %\n","Accuracy of the network on 5824 cifar-10 test images: 9.958791 %\n","Accuracy of the network on 6464 cifar-10 test images: 9.823639 %\n","Accuracy of the network on 7104 cifar-10 test images: 9.867680 %\n","Accuracy of the network on 7744 cifar-10 test images: 9.930269 %\n","Accuracy of the network on 8384 cifar-10 test images: 9.983302 %\n","Accuracy of the network on 9024 cifar-10 test images: 10.039894 %\n","Accuracy of the network on 9664 cifar-10 test images: 9.995861 %\n","Accuracy of the network on 10000 cifar-10 test images: 10.000000 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-WN5gQtpAfr","executionInfo":{"status":"ok","timestamp":1618945025028,"user_tz":240,"elapsed":32968,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"debe0b0f-5ed3-4356-dd40-e8e664d19b74"},"source":["# Epoch 1 V2 Model on Original test data\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","  for i, (inputs, labels) in enumerate(cifar_testloader):\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    if(i % 10 == 0):\n","      print('Accuracy of the network on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy of the network on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ATTA_ep5_Orig_Acc\"] = 100 * correct / total"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy of the network on 64 cifar-10 test images: 9.375000 %\n","Accuracy of the network on 704 cifar-10 test images: 10.369318 %\n","Accuracy of the network on 1344 cifar-10 test images: 10.416667 %\n","Accuracy of the network on 1984 cifar-10 test images: 9.879032 %\n","Accuracy of the network on 2624 cifar-10 test images: 9.984756 %\n","Accuracy of the network on 3264 cifar-10 test images: 9.957108 %\n","Accuracy of the network on 3904 cifar-10 test images: 9.810451 %\n","Accuracy of the network on 4544 cifar-10 test images: 9.925176 %\n","Accuracy of the network on 5184 cifar-10 test images: 9.780093 %\n","Accuracy of the network on 5824 cifar-10 test images: 9.872940 %\n","Accuracy of the network on 6464 cifar-10 test images: 9.962871 %\n","Accuracy of the network on 7104 cifar-10 test images: 10.050676 %\n","Accuracy of the network on 7744 cifar-10 test images: 10.020661 %\n","Accuracy of the network on 8384 cifar-10 test images: 9.947519 %\n","Accuracy of the network on 9024 cifar-10 test images: 9.929078 %\n","Accuracy of the network on 9664 cifar-10 test images: 9.923427 %\n","Accuracy of the network on 10000 cifar-10 test images: 10.000000 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Akvevp9pZy-E"},"source":["# Epoch 2 Model on Original test data\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","  for i, (inputs, labels) in enumerate(cifar_testloader):\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    if(i % 10 == 0):\n","      print('Accuracy of the network on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy of the network on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ATTA_ep5_Orig_Acc\"] = 100 * correct / total"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HDqTf0pbwDPd","executionInfo":{"status":"ok","timestamp":1618939430873,"user_tz":240,"elapsed":32783,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"4b413c11-069a-4b97-9881-eef5724268bf"},"source":["# Epoch 5 Model on Original test data\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","  for i, (inputs, labels) in enumerate(cifar_testloader):\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    if(i % 10 == 0):\n","      print('Accuracy of the network on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy of the network on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ATTA_ep5_Orig_Acc\"] = 100 * correct / total"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy of the network on 64 cifar-10 test images: 12.500000 %\n","Accuracy of the network on 704 cifar-10 test images: 13.494318 %\n","Accuracy of the network on 1344 cifar-10 test images: 14.955357 %\n","Accuracy of the network on 1984 cifar-10 test images: 15.020161 %\n","Accuracy of the network on 2624 cifar-10 test images: 15.129573 %\n","Accuracy of the network on 3264 cifar-10 test images: 15.073529 %\n","Accuracy of the network on 3904 cifar-10 test images: 14.959016 %\n","Accuracy of the network on 4544 cifar-10 test images: 14.920775 %\n","Accuracy of the network on 5184 cifar-10 test images: 14.872685 %\n","Accuracy of the network on 5824 cifar-10 test images: 15.024038 %\n","Accuracy of the network on 6464 cifar-10 test images: 15.006188 %\n","Accuracy of the network on 7104 cifar-10 test images: 15.047860 %\n","Accuracy of the network on 7744 cifar-10 test images: 14.914773 %\n","Accuracy of the network on 8384 cifar-10 test images: 14.957061 %\n","Accuracy of the network on 9024 cifar-10 test images: 14.882535 %\n","Accuracy of the network on 9664 cifar-10 test images: 14.879967 %\n","Accuracy of the network on 10000 cifar-10 test images: 14.910000 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RFoBS8kMPoDU","executionInfo":{"status":"ok","timestamp":1618939692963,"user_tz":240,"elapsed":735,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"4e45bc13-7584-431b-d597-aea009216a71"},"source":["result_dict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Transformer': 14.91,\n"," 'ViT_3eps_ATTA_cifar10_sec': 9184.282611370087,\n"," 'ViT_ATTA_5_ep2_test_pgd10_acc': 13.45,\n"," 'ViT_ATTA_5_ep2_test_pgd20_acc': 13.12,\n"," 'ViT_ATTA_5_ep3_gen_atk_sec': 2409.723920583725,\n"," 'ViT_ATTA_5_ep3_train_acc': 22.488,\n"," 'ViT_ATTA_5_ep3_train_sec': 649.1576640605927,\n"," 'ViT_ATTA_5_ep4_gen_atk_sec': 2411.9986884593964,\n"," 'ViT_ATTA_5_ep4_train_acc': 15.032,\n"," 'ViT_ATTA_5_ep4_train_sec': 648.385701417923,\n"," 'ViT_ATTA_5_ep5_gen_atk_sec': 2400.8721063137054,\n"," 'ViT_ATTA_5_ep5_test_pgd10_acc': 14.68,\n"," 'ViT_ATTA_5_ep5_test_pgd20_acc': 14.31,\n"," 'ViT_ATTA_5_ep5_train_acc': 17.956,\n"," 'ViT_ATTA_5_ep5_train_sec': 650.7549364566803}"]},"metadata":{"tags":[]},"execution_count":21}]}]}