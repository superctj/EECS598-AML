{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"z_torchattack_ATTA_5_8/255_cifar10.ipynb","provenance":[{"file_id":"1XX5nol2FpsTSefwQgG1Ayv_UqqRwWgIb","timestamp":1618157737492}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"eda01cff9404437bab00a2957f895684":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cc72892fd3ee4cec93eb5f8decbee9d5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d966a45c5a474492a8a62e57aec77a5c","IPY_MODEL_fe84a5cb53584cafb481b61a44b45490"]}},"cc72892fd3ee4cec93eb5f8decbee9d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d966a45c5a474492a8a62e57aec77a5c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_06e45330b361488bba4db0beaf5829d7","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":170498071,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":170498071,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ec1fc1dcdbe448838af01ed3ee983e7a"}},"fe84a5cb53584cafb481b61a44b45490":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_059fbce9485f4b3f987dca896a8f1629","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170499072/? [00:34&lt;00:00, 4956526.83it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4e2135184ef446b08244d12054f9666a"}},"06e45330b361488bba4db0beaf5829d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ec1fc1dcdbe448838af01ed3ee983e7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"059fbce9485f4b3f987dca896a8f1629":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4e2135184ef446b08244d12054f9666a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f629a785e2214fdcb2ca4f06b0af0f8f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b2c4d1d2aa424ac59eac9cabdcb58f10","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1f8c620572a94b08a9f762e69d516e03","IPY_MODEL_c330181a757d4cad8b72b1aa2c060ede"]}},"b2c4d1d2aa424ac59eac9cabdcb58f10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1f8c620572a94b08a9f762e69d516e03":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6de9c38bbdd44db5a8aaf5a56af4089a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":918,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":918,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_af3e9dc779444ffd8fdd1d95d2bdbda4"}},"c330181a757d4cad8b72b1aa2c060ede":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ec611496b22041bda26079c1f7451017","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 918/918 [00:00&lt;00:00, 2.47kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cc007d34d50843fab6854c26f3101e83"}},"6de9c38bbdd44db5a8aaf5a56af4089a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"af3e9dc779444ffd8fdd1d95d2bdbda4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ec611496b22041bda26079c1f7451017":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cc007d34d50843fab6854c26f3101e83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f5836f06cfc94beb9bde61b22a21f6f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_47a2acb73227463e96eeabd4c871ce3a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d5d852ce0d2244168b1a4ab393d45f02","IPY_MODEL_f165161dc2e646cd8e75edb0ae1b93f8"]}},"47a2acb73227463e96eeabd4c871ce3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d5d852ce0d2244168b1a4ab393d45f02":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_745da05ebf5d4d82810a2f8978af35b9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":343306743,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":343306743,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_df27f1fe9d994b178d9640424244c51c"}},"f165161dc2e646cd8e75edb0ae1b93f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_132709820f84419399240f74d943c9a9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 343M/343M [00:06&lt;00:00, 49.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b250aa3a9feb476ba64cb6e98453996e"}},"745da05ebf5d4d82810a2f8978af35b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"df27f1fe9d994b178d9640424244c51c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"132709820f84419399240f74d943c9a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b250aa3a9feb476ba64cb6e98453996e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TJGSG89Uker4","executionInfo":{"status":"ok","timestamp":1619038100090,"user_tz":240,"elapsed":30275,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"8d34db67-81d5-4390-a473-4bca5aac4652"},"source":["!git clone https://github.com/nateraw/huggingface-vit-finetune.git\n","%cd huggingface-vit-finetune/\n","!pip install -r requirements.txt\n","!pip install git+https://github.com/huggingface/transformers.git@master --upgrade\n","!pip install torchattacks"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'huggingface-vit-finetune'...\n","remote: Enumerating objects: 24, done.\u001b[K\n","remote: Counting objects: 100% (24/24), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 24 (delta 10), reused 19 (delta 6), pack-reused 0\u001b[K\n","Unpacking objects: 100% (24/24), done.\n","/content/huggingface-vit-finetune\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.8.1+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.9.1+cu101)\n","Collecting pytorch-lightning\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/99/68da5c6ca999de560036d98c492e507d17996f5eeb7e76ba64acd4bbb142/pytorch_lightning-1.2.8-py3-none-any.whl (841kB)\n","\u001b[K     |████████████████████████████████| 849kB 9.0MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (1.19.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 2)) (7.1.2)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r requirements.txt (line 3)) (4.41.1)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r requirements.txt (line 3)) (2.4.1)\n","Collecting future>=0.17.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n","\u001b[K     |████████████████████████████████| 829kB 25.9MB/s \n","\u001b[?25hCollecting torchmetrics>=0.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/79/e0e5bd38def3b53cf7abf7cf94c90b5d64248892f153c20a0c7d337b927c/torchmetrics-0.3.0-py3-none-any.whl (270kB)\n","\u001b[K     |████████████████████████████████| 276kB 47.0MB/s \n","\u001b[?25hCollecting fsspec[http]>=0.8.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n","\u001b[K     |████████████████████████████████| 112kB 46.6MB/s \n","\u001b[?25hCollecting PyYAML!=5.4.*,>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n","\u001b[K     |████████████████████████████████| 276kB 49.8MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.12.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.0.1)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.36.2)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.32.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.12.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (54.2.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.3.4)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.15.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.28.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics>=0.2.0->pytorch-lightning->-r requirements.txt (line 3)) (20.9)\n","Collecting aiohttp; extra == \"http\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 52.5MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2020.12.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.10.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (1.3.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics>=0.2.0->pytorch-lightning->-r requirements.txt (line 3)) (2.4.7)\n","Collecting multidict<7.0,>=4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n","\u001b[K     |████████████████████████████████| 143kB 34.6MB/s \n","\u001b[?25hCollecting async-timeout<4.0,>=3.0\n","  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 3)) (20.3.0)\n","Collecting yarl<2.0,>=1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n","\u001b[K     |████████████████████████████████| 296kB 55.7MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.4.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 3)) (3.1.0)\n","Building wheels for collected packages: future, PyYAML\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=7e95a3ce3cd4f94d7955f2de0358af9329d423b220b9f18a1bc17bbb4a70851d\n","  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n","  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=04d8841689609340f5325582bd4097262a5c47d0311e30f679b010d16cb126b6\n","  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n","Successfully built future PyYAML\n","Installing collected packages: future, torchmetrics, multidict, async-timeout, yarl, aiohttp, fsspec, PyYAML, pytorch-lightning\n","  Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.3.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.4.0 future-0.18.2 multidict-5.1.0 pytorch-lightning-1.2.8 torchmetrics-0.3.0 yarl-1.6.3\n","Collecting git+https://github.com/huggingface/transformers.git@master\n","  Cloning https://github.com/huggingface/transformers.git (to revision master) to /tmp/pip-req-build-eqddbsp3\n","  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-eqddbsp3\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 7.2MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n","Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 11.3MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n","Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n","Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n","Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.10.1)\n","Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n","Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n","Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n","Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2112847 sha256=3c085e1a38cc98e537bedd79e2b28f03848aa1e834be4dee7e0f4daad9c9e1ea\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-gg29p3bk/wheels/03/01/00/f2c9020459e177a2729b25f0b9628d95bb967727d71a118170\n","Successfully built transformers\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.0.dev0\n","Collecting torchattacks\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/a8/559ba77713a870f3d288833455091753435daad414de5ccce64812199654/torchattacks-2.14.1-py3-none-any.whl (92kB)\n","\u001b[K     |████████████████████████████████| 102kB 7.2MB/s \n","\u001b[?25hInstalling collected packages: torchattacks\n","Successfully installed torchattacks-2.14.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c_fWRmZTkvTT","executionInfo":{"status":"ok","timestamp":1619038113468,"user_tz":240,"elapsed":4108,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from transformers import ViTFeatureExtractor, ViTForImageClassification\n","import requests\n","\n","import torchattacks\n","import time\n","import json"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zrXKbxgilRQd","executionInfo":{"status":"ok","timestamp":1619038116191,"user_tz":240,"elapsed":308,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"ff1a33f1-0cbf-4a6a-de92-f5d10d194e20"},"source":["result_dict = {}\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ReR5Gilddzy","executionInfo":{"status":"ok","timestamp":1619038135390,"user_tz":240,"elapsed":17985,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"3fbcb76d-c524-466c-da5f-6f4d927cf66e"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136,"referenced_widgets":["eda01cff9404437bab00a2957f895684","cc72892fd3ee4cec93eb5f8decbee9d5","d966a45c5a474492a8a62e57aec77a5c","fe84a5cb53584cafb481b61a44b45490","06e45330b361488bba4db0beaf5829d7","ec1fc1dcdbe448838af01ed3ee983e7a","059fbce9485f4b3f987dca896a8f1629","4e2135184ef446b08244d12054f9666a"]},"id":"XK6VtyXRlhCa","executionInfo":{"status":"ok","timestamp":1619038151959,"user_tz":240,"elapsed":7510,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"b0e7980e-b9da-46c0-aeb8-145e4b097fb5"},"source":["transform = transforms.Compose(\n","    [\n","      # transforms.Resize((224, 224)),\n","      transforms.ToTensor(),\n","      # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","cifar_trainloader = torch.utils.data.DataLoader(cifar_trainset, batch_size=64, shuffle=True, num_workers=2)\n","cifar_testloader = torch.utils.data.DataLoader(cifar_testset, batch_size=64, shuffle=False, num_workers=2)\n","cifar_classes = [\n","    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n","]"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eda01cff9404437bab00a2957f895684","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hsjqC0seGppe","executionInfo":{"status":"ok","timestamp":1619038157783,"user_tz":240,"elapsed":295,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}}},"source":["class Transformer(nn.Module):\n","  def __init__(self, model):\n","    super(Transformer, self).__init__()\n","    self.model = model\n","  def forward(self, x):\n","    return self.model(x)[0]\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"cJILmMrBoDg2","colab":{"base_uri":"https://localhost:8080/","height":114,"referenced_widgets":["f629a785e2214fdcb2ca4f06b0af0f8f","b2c4d1d2aa424ac59eac9cabdcb58f10","1f8c620572a94b08a9f762e69d516e03","c330181a757d4cad8b72b1aa2c060ede","6de9c38bbdd44db5a8aaf5a56af4089a","af3e9dc779444ffd8fdd1d95d2bdbda4","ec611496b22041bda26079c1f7451017","cc007d34d50843fab6854c26f3101e83","f5836f06cfc94beb9bde61b22a21f6f5","47a2acb73227463e96eeabd4c871ce3a","d5d852ce0d2244168b1a4ab393d45f02","f165161dc2e646cd8e75edb0ae1b93f8","745da05ebf5d4d82810a2f8978af35b9","df27f1fe9d994b178d9640424244c51c","132709820f84419399240f74d943c9a9","b250aa3a9feb476ba64cb6e98453996e"]},"executionInfo":{"status":"ok","timestamp":1618952944260,"user_tz":240,"elapsed":16861,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"33d60a69-7789-4795-ace0-d55dce49fed6"},"source":["model = ViTForImageClassification.from_pretrained('nateraw/vit-base-patch16-224-cifar10', return_dict=False)\n","model.to(device)\n","model = nn.Sequential(\n","    nn.Upsample(scale_factor=7, mode='bilinear'),\n","    model\n",")\n","ViT_model = Transformer(model).to(device)\n"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f629a785e2214fdcb2ca4f06b0af0f8f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=918.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5836f06cfc94beb9bde61b22a21f6f5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=343306743.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RJSiI8ms4hpx","executionInfo":{"status":"ok","timestamp":1618952944978,"user_tz":240,"elapsed":692,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}}},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(ViT_model.parameters(), lr=0.0002)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJ0zaBqHnDk1","executionInfo":{"status":"ok","timestamp":1618947884490,"user_tz":240,"elapsed":59474,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"dcb4e5ad-4e93-4acf-dd9a-30a46477f01b"},"source":["# cifar-10 DATASET consists of 60000 images\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","  for i, (inputs, labels) in enumerate(cifar_testloader):\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    if(i % 10 == 0):\n","      print('Accuracy of the network on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy of the network on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"Transformer\"] = 100 * correct / total"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy of the network on 64 cifar-10 test images: 17.187500 %\n","Accuracy of the network on 704 cifar-10 test images: 11.789773 %\n","Accuracy of the network on 1344 cifar-10 test images: 11.458333 %\n","Accuracy of the network on 1984 cifar-10 test images: 10.735887 %\n","Accuracy of the network on 2624 cifar-10 test images: 10.556402 %\n","Accuracy of the network on 3264 cifar-10 test images: 10.079657 %\n","Accuracy of the network on 3904 cifar-10 test images: 9.912910 %\n","Accuracy of the network on 4544 cifar-10 test images: 9.727113 %\n","Accuracy of the network on 5184 cifar-10 test images: 9.934414 %\n","Accuracy of the network on 5824 cifar-10 test images: 10.027473 %\n","Accuracy of the network on 6464 cifar-10 test images: 9.916460 %\n","Accuracy of the network on 7104 cifar-10 test images: 9.952140 %\n","Accuracy of the network on 7744 cifar-10 test images: 9.865702 %\n","Accuracy of the network on 8384 cifar-10 test images: 9.887882 %\n","Accuracy of the network on 9024 cifar-10 test images: 10.095301 %\n","Accuracy of the network on 9664 cifar-10 test images: 10.078642 %\n","Accuracy of the network on 10000 cifar-10 test images: 10.000000 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"I6FpJ63mb-Xu","executionInfo":{"status":"error","timestamp":1618944202068,"user_tz":240,"elapsed":94536,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"6ca77c3a-976e-487e-d1f6-06e6ff8c4e06"},"source":["correct = 0\n","total = 0\n","with torch.no_grad():\n","  for i, (inputs, labels) in enumerate(cifar_trainloader):\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    if(i % 10 == 0):\n","      print('Accuracy of the network on %d cifar-10 train images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy of the network on %d cifar-10 train images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"Transformer\"] = 100 * correct / total"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy of the network on 64 cifar-10 train images: 100.000000 %\n","Accuracy of the network on 704 cifar-10 train images: 100.000000 %\n","Accuracy of the network on 1344 cifar-10 train images: 100.000000 %\n","Accuracy of the network on 1984 cifar-10 train images: 100.000000 %\n","Accuracy of the network on 2624 cifar-10 train images: 100.000000 %\n","Accuracy of the network on 3264 cifar-10 train images: 100.000000 %\n","Accuracy of the network on 3904 cifar-10 train images: 100.000000 %\n","Accuracy of the network on 4544 cifar-10 train images: 99.977993 %\n","Accuracy of the network on 5184 cifar-10 train images: 99.980710 %\n","Accuracy of the network on 5824 cifar-10 train images: 99.965659 %\n","Accuracy of the network on 6464 cifar-10 train images: 99.969059 %\n","Accuracy of the network on 7104 cifar-10 train images: 99.971847 %\n","Accuracy of the network on 7744 cifar-10 train images: 99.974174 %\n","Accuracy of the network on 8384 cifar-10 train images: 99.976145 %\n","Accuracy of the network on 9024 cifar-10 train images: 99.977837 %\n","Accuracy of the network on 9664 cifar-10 train images: 99.979305 %\n","Accuracy of the network on 10304 cifar-10 train images: 99.980590 %\n","Accuracy of the network on 10944 cifar-10 train images: 99.972588 %\n","Accuracy of the network on 11584 cifar-10 train images: 99.974102 %\n","Accuracy of the network on 12224 cifar-10 train images: 99.975458 %\n","Accuracy of the network on 12864 cifar-10 train images: 99.976679 %\n","Accuracy of the network on 13504 cifar-10 train images: 99.977784 %\n","Accuracy of the network on 14144 cifar-10 train images: 99.978790 %\n","Accuracy of the network on 14784 cifar-10 train images: 99.979708 %\n","Accuracy of the network on 15424 cifar-10 train images: 99.980550 %\n","Accuracy of the network on 16064 cifar-10 train images: 99.981325 %\n","Accuracy of the network on 16704 cifar-10 train images: 99.982040 %\n","Accuracy of the network on 17344 cifar-10 train images: 99.982703 %\n","Accuracy of the network on 17984 cifar-10 train images: 99.977758 %\n","Accuracy of the network on 18624 cifar-10 train images: 99.978522 %\n","Accuracy of the network on 19264 cifar-10 train images: 99.979236 %\n","Accuracy of the network on 19904 cifar-10 train images: 99.979904 %\n","Accuracy of the network on 20544 cifar-10 train images: 99.980530 %\n","Accuracy of the network on 21184 cifar-10 train images: 99.981118 %\n","Accuracy of the network on 21824 cifar-10 train images: 99.981672 %\n","Accuracy of the network on 22464 cifar-10 train images: 99.982194 %\n","Accuracy of the network on 23104 cifar-10 train images: 99.982687 %\n","Accuracy of the network on 23744 cifar-10 train images: 99.983154 %\n","Accuracy of the network on 24384 cifar-10 train images: 99.983596 %\n","Accuracy of the network on 25024 cifar-10 train images: 99.984015 %\n","Accuracy of the network on 25664 cifar-10 train images: 99.984414 %\n","Accuracy of the network on 26304 cifar-10 train images: 99.984793 %\n","Accuracy of the network on 26944 cifar-10 train images: 99.985154 %\n","Accuracy of the network on 27584 cifar-10 train images: 99.985499 %\n","Accuracy of the network on 28224 cifar-10 train images: 99.985828 %\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-04bef7e41520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy of the network on %d cifar-10 train images: %f %%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":434},"id":"Rd5NEZ6iuKZy","executionInfo":{"status":"error","timestamp":1618949969388,"user_tz":240,"elapsed":239368,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"327f24f8-3bec-4061-9a06-5b91009cb989"},"source":["t0 = time.time()\n","\n","i = 0\n","correct = 0\n","total = 0\n","running_loss = 0.0\n","for (inputs, labels) in cifar_trainloader:\n","  # inputs & labels already on device\n","  # Adv training\n","  optimizer.zero_grad()\n","  inputs = inputs.to(device)\n","  labels = labels.to(device)\n","\n","  # forward + backward + optimize\n","  outputs = ViT_model(inputs)\n","  loss = criterion(outputs, labels)\n","  loss.backward()\n","  optimizer.step()\n","  running_loss += loss.item()\n","  \n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs.data, 1)\n","  total += labels.size(0)\n","  if i == 0:\n","    print(predicted, labels, total)\n","  correct += (predicted == labels).sum().item()\n","  # print(\"Here\", np.sum(predicted == labels))\n","  if i % 150 == 0:\n","    print('Training accuracy of ViT on %d clean train data: %f %%' % (total, 100 * correct / total))\n","    print(\"running loss: \", running_loss)\n","  i += 1\n","\n","print('Training accuracy on %d clean cifar-10 train images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ep1_train_acc\"] = 100 * correct / total\n","t1 = time.time()\n","result_dict[\"ViT_ep1_train_sec\"] = t1 - t0\n","result_dict"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["tensor([7, 2, 0, 4, 0, 8, 6, 8, 0, 0, 5, 7, 0, 8, 1, 3, 4, 5, 3, 0, 4, 0, 1, 0,\n","        4, 1, 2, 6, 8, 3, 6, 8, 0, 1, 1, 8, 3, 2, 5, 4, 7, 7, 7, 3, 5, 2, 5, 0,\n","        7, 4, 3, 6, 8, 1, 5, 8, 7, 8, 1, 0, 0, 0, 0, 5], device='cuda:0') tensor([7, 2, 0, 4, 0, 8, 6, 8, 9, 0, 5, 7, 8, 8, 9, 3, 4, 5, 3, 9, 4, 9, 1, 9,\n","        4, 1, 2, 6, 8, 3, 6, 8, 0, 1, 1, 8, 3, 2, 5, 4, 7, 7, 7, 3, 5, 2, 5, 9,\n","        7, 4, 3, 6, 8, 1, 5, 8, 7, 8, 1, 0, 9, 0, 8, 5], device='cuda:0') 64\n","Training accuracy of ViT on 64 clean train data: 85.937500 %\n","running loss:  0.005255349911749363\n","Training accuracy of ViT on 9664 clean train data: 98.727235 %\n","running loss:  22.896505969576538\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-6e7c9f058871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0;31m# print(\"Here\", np.sum(predicted == labels))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m150\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"yFCXMB3kO0n-"},"source":["# ATTA Prepare Evaluation Adversarial Examples\n"]},{"cell_type":"code","metadata":{"id":"NoMHnfYo77HR","executionInfo":{"status":"ok","timestamp":1618952956553,"user_tz":240,"elapsed":387,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}}},"source":["atk_pgd_5 = torchattacks.PGD(ViT_model, eps=8/255, alpha=2/255, steps=5)\n","atk_pgd_10 = torchattacks.PGD(ViT_model, eps=8/255, alpha=2/255, steps=10)\n","atk_pgd_20 = torchattacks.PGD(ViT_model, eps=8/255, alpha=2/255, steps=20)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_HFDvefMJm-","executionInfo":{"status":"ok","timestamp":1618954022986,"user_tz":240,"elapsed":981582,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"55243d89-3d1a-4e5b-fa10-5f7ddf8061e9"},"source":["cifar_adv_test10 = []\n","start = time.time()\n","for i, (inputs, labels) in enumerate(cifar_testloader):\n","    adv_images = atk_pgd_10(inputs, labels)\n","    cifar_adv_test10.append((adv_images.cpu(), labels.cpu()))\n","    if i % 100 == 0:\n","      print(\"Progress(%): \", i*64/100)\n","      \n","print(\"total time(sec) : %.2f\" % (time.time() - start))\n","\n","# Store pgd 10 attacks\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd10_8p_adv_test.pt'\n","!touch $outfile_name\n","torch.save(cifar_adv_test10, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Progress(%):  0.0\n","Progress(%):  64.0\n","total time(sec) : 979.84\n","ATTA_5_adv_itr1_model.pt  pgd_5_adv_itr1.pt\t      pgd_5_adv_itr4_model.pt\n","ATTA_5_adv_itr2_model.pt  pgd_5_adv_itr1_v2_model.pt  pgd_5_adv_itr4.pt\n","pgd10_8p_adv_test.pt\t  pgd_5_adv_itr1_v2.pt\t      pgd_5_adv_itr5_model.pt\n","pgd_10_adv_test.pt\t  pgd_5_adv_itr2_model.pt     pgd_5_adv_itr5.pt\n","pgd_20_adv_test.pt\t  pgd_5_adv_itr2.pt\t      README.gdoc\n","pgd_5_adv_itr1.json\t  pgd_5_adv_itr3_model.pt\n","pgd_5_adv_itr1_model.pt   pgd_5_adv_itr3.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vN9-RvK6DpPr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619040087028,"user_tz":240,"elapsed":2110,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"b21dd9c0-1cb3-4759-f0f3-331aca17cfd3"},"source":["# Load pgd 10 attacks\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd10_8p_adv_test.pt'\n","cifar_adv_test10 = torch.load(outfile_name)\n","len(cifar_adv_test10)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["157"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"ruBObKZHMc-M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618956690179,"user_tz":240,"elapsed":1963183,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"deb20d0a-366d-4ade-f1e1-15bbe517a683"},"source":["cifar_adv_test20 = []\n","start = time.time()\n","for i, (inputs, labels) in enumerate(cifar_testloader):\n","    adv_images = atk_pgd_20(inputs, labels)\n","    cifar_adv_test20.append((adv_images.cpu(), labels.cpu()))\n","    if i % 100 == 0:\n","      print(\"Progress(%): \", i*64/100)\n","\n","print(\"total time(sec) : %.2f\" % (time.time() - start))\n","\n","# Store pgd 20 attacks\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd20_8p_adv_test.pt'\n","!touch $outfile_name\n","torch.save(cifar_adv_test20, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Progress(%):  0.0\n","Progress(%):  64.0\n","total time(sec) : 1962.12\n","ATTA_5_adv_itr1_model.pt  pgd_5_adv_itr1_model.pt     pgd_5_adv_itr3.pt\n","ATTA_5_adv_itr2_model.pt  pgd_5_adv_itr1.pt\t      pgd_5_adv_itr4_model.pt\n","pgd10_8p_adv_test.pt\t  pgd_5_adv_itr1_v2_model.pt  pgd_5_adv_itr4.pt\n","pgd_10_adv_test.pt\t  pgd_5_adv_itr1_v2.pt\t      pgd_5_adv_itr5_model.pt\n","pgd20_8p_adv_test.pt\t  pgd_5_adv_itr2_model.pt     pgd_5_adv_itr5.pt\n","pgd_20_adv_test.pt\t  pgd_5_adv_itr2.pt\t      README.gdoc\n","pgd_5_adv_itr1.json\t  pgd_5_adv_itr3_model.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q9noMPXdDz2e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619040092855,"user_tz":240,"elapsed":3207,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"32b29c47-2e3c-418e-907b-31b790e255c0"},"source":["# Load pgd 20 attacks\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd20_8p_adv_test.pt'\n","cifar_adv_test20 = torch.load(outfile_name)\n","len(cifar_adv_test20)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["157"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"lxzg8NbBpuDK"},"source":["# ATTA Training Epoch 1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kPmXSSwCZtvG","executionInfo":{"status":"ok","timestamp":1618959210753,"user_tz":240,"elapsed":2461484,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"f2b6c57c-538e-48d8-ba22-7f05d4f673fe"},"source":["# Generate first itr adv images\n","cifar_adv = []\n","start = time.time()\n","for i, (inputs, labels) in enumerate(cifar_trainloader):\n","    adv_images = atk_pgd_5(inputs, labels)\n","    cifar_adv.append((adv_images.cpu(), labels.cpu()))\n","    if i % 100 == 0:\n","      print(\"Progress(%): \", 64*i/500)\n","\n","print(\"total time(sec) : %.2f\" % (time.time() - start))\n","\n","# Store\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd5_8p_adv_itr1.pt'\n","!touch $outfile_name\n","torch.save(cifar_adv, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Progress(%):  0.0\n","Progress(%):  12.8\n","Progress(%):  25.6\n","Progress(%):  38.4\n","Progress(%):  51.2\n","Progress(%):  64.0\n","Progress(%):  76.8\n","Progress(%):  89.6\n","total time(sec) : 2458.99\n","ATTA_5_adv_itr1_model.pt  pgd_5_adv_itr1.json\t      pgd_5_adv_itr3_model.pt\n","ATTA_5_adv_itr2_model.pt  pgd_5_adv_itr1_model.pt     pgd_5_adv_itr3.pt\n","pgd10_8p_adv_test.pt\t  pgd_5_adv_itr1.pt\t      pgd_5_adv_itr4_model.pt\n","pgd_10_adv_test.pt\t  pgd_5_adv_itr1_v2_model.pt  pgd_5_adv_itr4.pt\n","pgd20_8p_adv_test.pt\t  pgd_5_adv_itr1_v2.pt\t      pgd_5_adv_itr5_model.pt\n","pgd_20_adv_test.pt\t  pgd_5_adv_itr2_model.pt     pgd_5_adv_itr5.pt\n","pgd5_8p_adv_itr1.pt\t  pgd_5_adv_itr2.pt\t      README.gdoc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XH8fpZHm1lp3","executionInfo":{"status":"ok","timestamp":1618950212225,"user_tz":240,"elapsed":10954,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"39e7c43c-695a-4e41-c871-55134f767c7f"},"source":["# Load\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd5_8p_adv_itr1.pt'\n","cifar_adv = torch.load(outfile_name)\n","len(cifar_adv)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["782"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QG1R8Npl9iOB","executionInfo":{"status":"ok","timestamp":1618960073612,"user_tz":240,"elapsed":665185,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"db6fc373-6dd5-4e52-f7dc-e1fd5d9db1c9"},"source":["t0 = time.time()\n","\n","i = 0\n","correct = 0\n","total = 0\n","running_loss = 0.0\n","for (inputs, labels) in cifar_adv:\n","  # inputs & labels already on device\n","  # Adv training\n","  optimizer.zero_grad()\n","  inputs = inputs.to(device)\n","  labels = labels.to(device)\n","\n","  # forward + backward + optimize\n","  outputs = ViT_model(inputs)\n","  loss = criterion(outputs, labels)\n","  loss.backward()\n","  optimizer.step()\n","  running_loss += loss.item()\n","  \n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs.data, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  # print(\"Here\", np.sum(predicted == labels))\n","  if i % 150 == 0:\n","    print('Training accuracy of ViT on %d adv exs: %f %%' % (total, 100 * correct / total))\n","    print(\"running loss: \", running_loss)\n","  i += 1\n","\n","print('Training accuracy on %d pgd-5 cifar-10 adv images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep1_train_acc\"] = 100 * correct / total\n","t1 = time.time()\n","result_dict[\"ViT_ATTA_5_ep1_train_sec\"] = t1 - t0\n","result_dict\n","\n","# Store model after itr1\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'ATTA5_8p_adv_itr1_model.pt'\n","!touch $outfile_name\n","torch.save(ViT_model, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/"],"execution_count":13,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy of ViT on 64 adv exs: 25.000000 %\n","running loss:  6.625616550445557\n","Training accuracy of ViT on 9664 adv exs: 86.961921 %\n","running loss:  104.10059168934822\n","Training accuracy of ViT on 19264 adv exs: 90.297965 %\n","running loss:  161.9505341053009\n","Training accuracy of ViT on 28864 adv exs: 91.868764 %\n","running loss:  211.75836592912674\n","Training accuracy of ViT on 38464 adv exs: 92.853057 %\n","running loss:  256.01912017166615\n","Training accuracy of ViT on 48064 adv exs: 93.652214 %\n","running loss:  293.8845605030656\n","Training accuracy on 50000 pgd-5 cifar-10 adv images: 93.790000 %\n","ATTA5_8p_adv_itr1_model.pt  pgd_5_adv_itr1.json\t\tpgd_5_adv_itr3.pt\n","ATTA_5_adv_itr1_model.pt    pgd_5_adv_itr1_model.pt\tpgd_5_adv_itr4_model.pt\n","ATTA_5_adv_itr2_model.pt    pgd_5_adv_itr1.pt\t\tpgd_5_adv_itr4.pt\n","pgd10_8p_adv_test.pt\t    pgd_5_adv_itr1_v2_model.pt\tpgd_5_adv_itr5_model.pt\n","pgd_10_adv_test.pt\t    pgd_5_adv_itr1_v2.pt\tpgd_5_adv_itr5.pt\n","pgd20_8p_adv_test.pt\t    pgd_5_adv_itr2_model.pt\tREADME.gdoc\n","pgd_20_adv_test.pt\t    pgd_5_adv_itr2.pt\n","pgd5_8p_adv_itr1.pt\t    pgd_5_adv_itr3_model.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WIsNSsyoB90y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619038179692,"user_tz":240,"elapsed":11280,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"ddc9518f-e628-4cc4-a04c-837b036da7c4"},"source":["# Load model\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'ATTA5_8p_adv_itr1_model.pt'\n","ViT_model = torch.load(outfile_name)\n","ViT_model.to(device)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Transformer(\n","  (model): Sequential(\n","    (0): Upsample(scale_factor=7.0, mode=bilinear)\n","    (1): ViTForImageClassification(\n","      (vit): ViTModel(\n","        (embeddings): ViTEmbeddings(\n","          (patch_embeddings): PatchEmbeddings(\n","            (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","          )\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (encoder): ViTEncoder(\n","          (layer): ModuleList(\n","            (0): ViTLayer(\n","              (attention): ViTAttention(\n","                (attention): ViTSelfAttention(\n","                  (query): Linear(in_features=768, out_features=768, bias=True)\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): ViTSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (intermediate): ViTIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              )\n","              (output): ViTOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (1): ViTLayer(\n","              (attention): ViTAttention(\n","                (attention): ViTSelfAttention(\n","                  (query): Linear(in_features=768, out_features=768, bias=True)\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): ViTSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (intermediate): ViTIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              )\n","              (output): ViTOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (2): ViTLayer(\n","              (attention): ViTAttention(\n","                (attention): ViTSelfAttention(\n","                  (query): Linear(in_features=768, out_features=768, bias=True)\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): ViTSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (intermediate): ViTIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              )\n","              (output): ViTOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (3): ViTLayer(\n","              (attention): ViTAttention(\n","                (attention): ViTSelfAttention(\n","                  (query): Linear(in_features=768, out_features=768, bias=True)\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): ViTSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (intermediate): ViTIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              )\n","              (output): ViTOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (4): ViTLayer(\n","              (attention): ViTAttention(\n","                (attention): ViTSelfAttention(\n","                  (query): Linear(in_features=768, out_features=768, bias=True)\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): ViTSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (intermediate): ViTIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              )\n","              (output): ViTOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (5): ViTLayer(\n","              (attention): ViTAttention(\n","                (attention): ViTSelfAttention(\n","                  (query): Linear(in_features=768, out_features=768, bias=True)\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): ViTSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (intermediate): ViTIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              )\n","              (output): ViTOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (6): ViTLayer(\n","              (attention): ViTAttention(\n","                (attention): ViTSelfAttention(\n","                  (query): Linear(in_features=768, out_features=768, bias=True)\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): ViTSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (intermediate): ViTIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              )\n","              (output): ViTOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (7): ViTLayer(\n","              (attention): ViTAttention(\n","                (attention): ViTSelfAttention(\n","                  (query): Linear(in_features=768, out_features=768, bias=True)\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): ViTSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (intermediate): ViTIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              )\n","              (output): ViTOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (8): ViTLayer(\n","              (attention): ViTAttention(\n","                (attention): ViTSelfAttention(\n","                  (query): Linear(in_features=768, out_features=768, bias=True)\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): ViTSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (intermediate): ViTIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              )\n","              (output): ViTOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (9): ViTLayer(\n","              (attention): ViTAttention(\n","                (attention): ViTSelfAttention(\n","                  (query): Linear(in_features=768, out_features=768, bias=True)\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): ViTSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (intermediate): ViTIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              )\n","              (output): ViTOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (10): ViTLayer(\n","              (attention): ViTAttention(\n","                (attention): ViTSelfAttention(\n","                  (query): Linear(in_features=768, out_features=768, bias=True)\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): ViTSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (intermediate): ViTIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              )\n","              (output): ViTOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","            (11): ViTLayer(\n","              (attention): ViTAttention(\n","                (attention): ViTSelfAttention(\n","                  (query): Linear(in_features=768, out_features=768, bias=True)\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): ViTSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (intermediate): ViTIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              )\n","              (output): ViTOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            )\n","          )\n","        )\n","        (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (classifier): Linear(in_features=768, out_features=10, bias=True)\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CpPPxTh6jLlJ","executionInfo":{"status":"ok","timestamp":1618960238098,"user_tz":240,"elapsed":435,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"7a2da40a-14a0-4c3b-ee8e-f877d6c6eacf"},"source":["result_dict"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ViT_ATTA_5_ep1_train_acc': 93.79,\n"," 'ViT_ATTA_5_ep1_train_sec': 663.3380136489868}"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"BnnuCByzqILw"},"source":["# Evaluate ATTA Epoch 1 Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhF0N5CsZggK","executionInfo":{"status":"ok","timestamp":1618960399122,"user_tz":240,"elapsed":33530,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"31b8c6e7-e965-40ac-90c0-d4a1e7ccdc04"},"source":["# Epoch 1 Model on Original test data\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","  for i, (inputs, labels) in enumerate(cifar_testloader):\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    if(i % 50 == 0):\n","      print('Accuracy of the network on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy of the network on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ATTA_ep1_Orig_Acc\"] = 100 * correct / total"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy of the network on 64 cifar-10 test images: 93.750000 %\n","Accuracy of the network on 704 cifar-10 test images: 92.897727 %\n","Accuracy of the network on 1344 cifar-10 test images: 93.154762 %\n","Accuracy of the network on 1984 cifar-10 test images: 92.691532 %\n","Accuracy of the network on 2624 cifar-10 test images: 92.530488 %\n","Accuracy of the network on 3264 cifar-10 test images: 93.075980 %\n","Accuracy of the network on 3904 cifar-10 test images: 92.776639 %\n","Accuracy of the network on 4544 cifar-10 test images: 93.001761 %\n","Accuracy of the network on 5184 cifar-10 test images: 93.055556 %\n","Accuracy of the network on 5824 cifar-10 test images: 93.131868 %\n","Accuracy of the network on 6464 cifar-10 test images: 93.177599 %\n","Accuracy of the network on 7104 cifar-10 test images: 93.172860 %\n","Accuracy of the network on 7744 cifar-10 test images: 93.168905 %\n","Accuracy of the network on 8384 cifar-10 test images: 93.093989 %\n","Accuracy of the network on 9024 cifar-10 test images: 93.151596 %\n","Accuracy of the network on 9664 cifar-10 test images: 93.170530 %\n","Accuracy of the network on 10000 cifar-10 test images: 93.210000 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DOsSqgaB066g","executionInfo":{"status":"ok","timestamp":1618960338257,"user_tz":240,"elapsed":32995,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"0d1f642f-b72c-40b7-bc19-5b789cf9f8cd"},"source":["# Evaluate model on pgd-10 test adv\n","correct = 0\n","total = 0\n","for i, (inputs, labels) in enumerate(cifar_adv_test10):\n","  inputs, labels = inputs.to(device), labels.to(device)\n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  if(i % 100 == 0):\n","    print('Accuracy on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy ATTA5-ep1 ViT on %d pgd10 cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep1_test_pgd10_acc\"] = 100 * correct / total"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy on 64 cifar-10 test images: 87.500000 %\n","Accuracy on 6464 cifar-10 test images: 91.321163 %\n","Accuracy ATTA5-ep1 ViT on 10000 cifar-10 test images: 91.290000 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"myBc4RbQHzwc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618960480135,"user_tz":240,"elapsed":33037,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"d6825117-0ec0-4808-991b-1122d11a39ec"},"source":["# Evaluate model on pgd-20 test adv\n","correct = 0\n","total = 0\n","for i, (inputs, labels) in enumerate(cifar_adv_test20):\n","  inputs, labels = inputs.to(device), labels.to(device)\n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  if(i % 100 == 0):\n","    print('Accuracy on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy ATTA5-ep1 ViT on %d pgd20 cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep1_test_pgd20_acc\"] = 100 * correct / total"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy on 64 cifar-10 test images: 89.062500 %\n","Accuracy on 6464 cifar-10 test images: 90.965347 %\n","Accuracy ATTA5-ep1 ViT on 10000 pgd20 cifar-10 test images: 91.070000 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3PXzH8sSpfux"},"source":["# ATTA Training Epoch 2\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kgck_3nWIQaj","executionInfo":{"status":"ok","timestamp":1618963071769,"user_tz":240,"elapsed":2460375,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"7f20871c-2eb1-4999-b77c-129397a42daa"},"source":["# ATTA generate attack ep2\n","i = 0\n","t0 = time.time()\n","for (inputs, labels) in cifar_adv:\n","  adv_images = atk_pgd_5(inputs, labels)\n","  cifar_adv[i] = (adv_images.cpu(), labels.cpu())\n","  if i % 100 == 0:\n","    print(\"Progress(%): \", 64*i/500)\n","  i += 1\n","\n","time_gen_atk_2 = time.time() - t0\n","print(\"time lap(sec):\", time_gen_atk_2)\n","\n","# Store\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd5_8p_adv_itr2.pt'\n","!touch $outfile_name\n","torch.save(cifar_adv, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/"],"execution_count":18,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Progress(%):  0.0\n","Progress(%):  12.8\n","Progress(%):  25.6\n","Progress(%):  38.4\n","Progress(%):  51.2\n","Progress(%):  64.0\n","Progress(%):  76.8\n","Progress(%):  89.6\n","time lap(sec): 2457.4898376464844\n","ATTA5_8p_adv_itr1_model.pt  pgd5_8p_adv_itr2.pt\t\tpgd_5_adv_itr3_model.pt\n","ATTA_5_adv_itr1_model.pt    pgd_5_adv_itr1.json\t\tpgd_5_adv_itr3.pt\n","ATTA_5_adv_itr2_model.pt    pgd_5_adv_itr1_model.pt\tpgd_5_adv_itr4_model.pt\n","pgd10_8p_adv_test.pt\t    pgd_5_adv_itr1.pt\t\tpgd_5_adv_itr4.pt\n","pgd_10_adv_test.pt\t    pgd_5_adv_itr1_v2_model.pt\tpgd_5_adv_itr5_model.pt\n","pgd20_8p_adv_test.pt\t    pgd_5_adv_itr1_v2.pt\tpgd_5_adv_itr5.pt\n","pgd_20_adv_test.pt\t    pgd_5_adv_itr2_model.pt\tREADME.gdoc\n","pgd5_8p_adv_itr1.pt\t    pgd_5_adv_itr2.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pj43NI3OYE8S","executionInfo":{"status":"ok","timestamp":1619038192907,"user_tz":240,"elapsed":11637,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"0a187bc3-f388-49a9-80bc-726d051f7b81"},"source":["# Load\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'pgd5_8p_adv_itr2.pt'\n","cifar_adv = torch.load(outfile_name)\n","len(cifar_adv)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["782"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfgYXuejXzPF","executionInfo":{"status":"ok","timestamp":1618964650928,"user_tz":240,"elapsed":668248,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"18970439-394f-47ea-f460-6a6817523007"},"source":["# ATTA train ep2\n","t0 = time.time()\n","\n","i = 0\n","correct = 0\n","total = 0\n","running_loss = 0.0\n","for (inputs, labels) in cifar_adv:\n","  # inputs & labels already on device\n","  # Adv training\n","  optimizer.zero_grad()\n","  inputs = inputs.to(device)\n","  labels = labels.to(device)\n","\n","  # forward + backward + optimize\n","  outputs = ViT_model(inputs)\n","  loss = criterion(outputs, labels)\n","  loss.backward()\n","  optimizer.step()\n","  running_loss += loss.item()\n","  \n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs.data, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  # print(\"Here\", np.sum(predicted == labels))\n","  if i % 150 == 0:\n","    print('Training accuracy of ViT on %d adv exs: %f %%' % (total, 100 * correct / total))\n","    print(\"running loss: \", running_loss)\n","  i += 1\n","\n","print('Training accuracy on %d pgd-5 cifar-10 adv images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep2_train_acc\"] = 100 * correct / total\n","t1 = time.time()\n","result_dict[\"ViT_ATTA_5_ep2_train_sec\"] = t1 - t0\n","\n","# Store model after itr2\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'ATTA5_8p_adv_itr2_model.pt'\n","!touch $outfile_name\n","torch.save(ViT_model, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/\n","\n","result_dict"],"execution_count":19,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy of ViT on 64 adv exs: 0.000000 %\n","running loss:  7.169466018676758\n","Training accuracy of ViT on 9664 adv exs: 82.740066 %\n","running loss:  115.25743697583675\n","Training accuracy of ViT on 19264 adv exs: 90.381022 %\n","running loss:  142.59680522605777\n","Training accuracy of ViT on 28864 adv exs: 93.164496 %\n","running loss:  163.8698865659535\n","Training accuracy of ViT on 38464 adv exs: 94.667741 %\n","running loss:  183.74595912732184\n","Training accuracy of ViT on 48064 adv exs: 95.568409 %\n","running loss:  201.96931966766715\n","Training accuracy on 50000 pgd-5 cifar-10 adv images: 95.700000 %\n","ATTA5_8p_adv_itr1_model.pt  pgd5_8p_adv_itr2.pt\t\tpgd_5_adv_itr3_model.pt\n","ATTA_5_adv_itr1_model.pt    pgd_5_adv_itr1.json\t\tpgd_5_adv_itr3.pt\n","ATTA_5_adv_itr2_model.pt    pgd_5_adv_itr1_model.pt\tpgd_5_adv_itr4_model.pt\n","pgd10_8p_adv_test.pt\t    pgd_5_adv_itr1.pt\t\tpgd_5_adv_itr4.pt\n","pgd_10_adv_test.pt\t    pgd_5_adv_itr1_v2_model.pt\tpgd_5_adv_itr5_model.pt\n","pgd20_8p_adv_test.pt\t    pgd_5_adv_itr1_v2.pt\tpgd_5_adv_itr5.pt\n","pgd_20_adv_test.pt\t    pgd_5_adv_itr2_model.pt\tREADME.gdoc\n","pgd5_8p_adv_itr1.pt\t    pgd_5_adv_itr2.pt\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'ATTA_ep1_Orig_Acc': 93.21,\n"," 'ViT_ATTA_5_ep1_test_pgd10_acc': 91.29,\n"," 'ViT_ATTA_5_ep1_test_pgd20_acc': 91.07,\n"," 'ViT_ATTA_5_ep1_train_acc': 93.79,\n"," 'ViT_ATTA_5_ep1_train_sec': 663.3380136489868,\n"," 'ViT_ATTA_5_ep2_train_acc': 95.7,\n"," 'ViT_ATTA_5_ep2_train_sec': 663.5174517631531}"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"whze2TG_Fah5","executionInfo":{"status":"ok","timestamp":1619037638438,"user_tz":240,"elapsed":663591,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"e8736f60-3e77-4c21-bc56-6a8aea81afbd"},"source":["optimizer = optim.Adam(ViT_model.parameters(), lr=0.0001)\n","criterion = nn.CrossEntropyLoss()\n","\n","# ATTA train ep2 with lower learning rate\n","t0 = time.time()\n","\n","i = 0\n","correct = 0\n","total = 0\n","running_loss = 0.0\n","for (inputs, labels) in cifar_adv:\n","  # inputs & labels already on device\n","  # Adv training\n","  optimizer.zero_grad()\n","  inputs = inputs.to(device)\n","  labels = labels.to(device)\n","\n","  # forward + backward + optimize\n","  outputs = ViT_model(inputs)\n","  loss = criterion(outputs, labels)\n","  loss.backward()\n","  optimizer.step()\n","  running_loss += loss.item()\n","  \n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs.data, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  # print(\"Here\", np.sum(predicted == labels))\n","  if i % 150 == 0:\n","    print('Training accuracy of ViT on %d adv exs: %f %%' % (total, 100 * correct / total))\n","    print(\"running loss: \", running_loss)\n","  i += 1\n","\n","print('Training accuracy on %d pgd-5 cifar-10 adv images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep2_train_acc\"] = 100 * correct / total\n","t1 = time.time()\n","result_dict[\"ViT_ATTA_5_ep2_train_sec\"] = t1 - t0\n","\n","# Store model after itr2\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'ATTA5_8p_adv_itr2_model_v2.pt'\n","!touch $outfile_name\n","torch.save(ViT_model, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/\n","\n","result_dict"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy of ViT on 64 adv exs: 3.125000 %\n","running loss:  7.169466018676758\n","Training accuracy of ViT on 9664 adv exs: 83.567881 %\n","running loss:  103.10189383476973\n","Training accuracy of ViT on 19264 adv exs: 90.915698 %\n","running loss:  124.51980127766728\n","Training accuracy of ViT on 28864 adv exs: 93.625277 %\n","running loss:  140.5249695573002\n","Training accuracy of ViT on 38464 adv exs: 95.018719 %\n","running loss:  154.42406614683568\n","Training accuracy of ViT on 48064 adv exs: 95.874251 %\n","running loss:  166.3265758799389\n","Training accuracy on 50000 pgd-5 cifar-10 adv images: 96.004000 %\n","ATTA5_8p_adv_itr1_model.pt     pgd5_8p_adv_itr5.pt\n","ATTA5_8p_adv_itr2_model.pt     pgd_5_adv_itr1.json\n","ATTA5_8p_adv_itr2_model_v2.pt  pgd_5_adv_itr1_model.pt\n","ATTA5_8p_adv_itr3_model.pt     pgd_5_adv_itr1.pt\n","ATTA5_8p_adv_itr4_model.pt     pgd_5_adv_itr1_v2_model.pt\n","ATTA5_8p_adv_itr5_model.pt     pgd_5_adv_itr1_v2.pt\n","ATTA_5_adv_itr1_model.pt       pgd_5_adv_itr2_model.pt\n","ATTA_5_adv_itr2_model.pt       pgd_5_adv_itr2.pt\n","pgd10_8p_adv_test.pt\t       pgd_5_adv_itr3_model.pt\n","pgd_10_adv_test.pt\t       pgd_5_adv_itr3.pt\n","pgd20_8p_adv_test.pt\t       pgd_5_adv_itr4_model.pt\n","pgd_20_adv_test.pt\t       pgd_5_adv_itr4.pt\n","pgd5_8p_adv_itr1.pt\t       pgd_5_adv_itr5_model.pt\n","pgd5_8p_adv_itr2.pt\t       pgd_5_adv_itr5.pt\n","pgd5_8p_adv_itr3.pt\t       README.gdoc\n","pgd5_8p_adv_itr4.pt\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'ViT_ATTA_5_ep2_train_acc': 96.004,\n"," 'ViT_ATTA_5_ep2_train_sec': 661.7895069122314}"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A4xWNNMNL8qh","executionInfo":{"status":"ok","timestamp":1619039359312,"user_tz":240,"elapsed":1156013,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"0ddbc5b7-e5e7-4833-842f-d2b45ae6195d"},"source":["optimizer = optim.Adam(ViT_model.parameters(), lr=0.0004)\n","criterion = nn.CrossEntropyLoss()\n","\n","# ATTA train ep2 with HIGHER learning rate\n","t0 = time.time()\n","\n","i = 0\n","correct = 0\n","total = 0\n","running_loss = 0.0\n","for (inputs, labels) in cifar_adv:\n","  # inputs & labels already on device\n","  # Adv training\n","  optimizer.zero_grad()\n","  inputs = inputs.to(device)\n","  labels = labels.to(device)\n","\n","  # forward + backward + optimize\n","  outputs = ViT_model(inputs)\n","  loss = criterion(outputs, labels)\n","  loss.backward()\n","  optimizer.step()\n","  running_loss += loss.item()\n","  \n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs.data, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  # print(\"Here\", np.sum(predicted == labels))\n","  if i % 150 == 0:\n","    print('Training accuracy of ViT on %d adv exs: %f %%' % (total, 100 * correct / total))\n","    print(\"running loss: \", running_loss)\n","  i += 1\n","\n","print('Training accuracy on %d pgd-5 cifar-10 adv images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep2_train_acc\"] = 100 * correct / total\n","t1 = time.time()\n","result_dict[\"ViT_ATTA_5_ep2_train_sec\"] = t1 - t0\n","\n","# Store model after itr2\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'ATTA5_8p_adv_itr2_model_v3.pt'\n","!touch $outfile_name\n","torch.save(ViT_model, outfile_name)\n","!ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/\n","\n","result_dict"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy of ViT on 64 adv exs: 39.062500 %\n","running loss:  7.169466018676758\n","Training accuracy of ViT on 9664 adv exs: 79.956540 %\n","running loss:  128.4574111700058\n","Training accuracy of ViT on 19264 adv exs: 88.797757 %\n","running loss:  167.62339539825916\n","Training accuracy of ViT on 28864 adv exs: 91.920732 %\n","running loss:  202.5880583152175\n","Training accuracy of ViT on 38464 adv exs: 93.500416 %\n","running loss:  234.90400163829327\n","Training accuracy of ViT on 48064 adv exs: 94.536451 %\n","running loss:  263.85921609960496\n","Training accuracy on 50000 pgd-5 cifar-10 adv images: 94.706000 %\n","ATTA5_8p_adv_itr1_model.pt     pgd5_8p_adv_itr4.pt\n","ATTA5_8p_adv_itr2_model.pt     pgd5_8p_adv_itr5.pt\n","ATTA5_8p_adv_itr2_model_v2.pt  pgd_5_adv_itr1.json\n","ATTA5_8p_adv_itr2_model_v3.pt  pgd_5_adv_itr1_model.pt\n","ATTA5_8p_adv_itr3_model.pt     pgd_5_adv_itr1.pt\n","ATTA5_8p_adv_itr4_model.pt     pgd_5_adv_itr1_v2_model.pt\n","ATTA5_8p_adv_itr5_model.pt     pgd_5_adv_itr1_v2.pt\n","ATTA_5_adv_itr1_model.pt       pgd_5_adv_itr2_model.pt\n","ATTA_5_adv_itr2_model.pt       pgd_5_adv_itr2.pt\n","pgd10_8p_adv_test.pt\t       pgd_5_adv_itr3_model.pt\n","pgd_10_adv_test.pt\t       pgd_5_adv_itr3.pt\n","pgd20_8p_adv_test.pt\t       pgd_5_adv_itr4_model.pt\n","pgd_20_adv_test.pt\t       pgd_5_adv_itr4.pt\n","pgd5_8p_adv_itr1.pt\t       pgd_5_adv_itr5_model.pt\n","pgd5_8p_adv_itr2.pt\t       pgd_5_adv_itr5.pt\n","pgd5_8p_adv_itr3.pt\t       README.gdoc\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'ViT_ATTA_5_ep2_train_acc': 94.706,\n"," 'ViT_ATTA_5_ep2_train_sec': 1154.379698753357}"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"GeYOrLd5p-qS"},"source":["# Load model\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'ATTA5_8p_adv_itr2_model.pt'\n","ViT_model = torch.load(outfile_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EDcohcWKStW"},"source":["# Load model v2\n","output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","outfile_name = output_dir + 'ATTA5_8p_adv_itr2_model_v2.pt'\n","ViT_model = torch.load(outfile_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EKkOvomSqO_V"},"source":["# Evaluate ATTA Epoch 2 Model\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y-4jr4f2kpJl","executionInfo":{"status":"ok","timestamp":1619040038466,"user_tz":240,"elapsed":59089,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"19970250-90f2-4def-c66b-24116dc17ae9"},"source":["# Epoch 2 Model on Original test data\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","  for i, (inputs, labels) in enumerate(cifar_testloader):\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    if(i % 50 == 0):\n","      print('Accuracy of the network on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy of the network on %d cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ATTA_ep2_Orig_Acc\"] = 100 * correct / total"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy of the network on 64 cifar-10 test images: 75.000000 %\n","Accuracy of the network on 3264 cifar-10 test images: 71.415441 %\n","Accuracy of the network on 6464 cifar-10 test images: 71.565594 %\n","Accuracy of the network on 9664 cifar-10 test images: 71.864652 %\n","Accuracy of the network on 10000 cifar-10 test images: 71.750000 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4pFyKFKUqUq3","executionInfo":{"status":"ok","timestamp":1619040159935,"user_tz":240,"elapsed":58616,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"8f692a01-f581-406b-f2d7-70ac7ddd2f44"},"source":["# Evaluate model on pgd-10 test adv\n","correct = 0\n","total = 0\n","for i, (inputs, labels) in enumerate(cifar_adv_test10):\n","  inputs, labels = inputs.to(device), labels.to(device)\n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  if(i % 100 == 0):\n","    print('Accuracy on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy ATTA5-ep2 ViT on %d pgd10 cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep2_test_pgd10_acc\"] = 100 * correct / total"],"execution_count":13,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy on 64 cifar-10 test images: 75.000000 %\n","Accuracy on 6464 cifar-10 test images: 67.636139 %\n","Accuracy ATTA5-ep2 ViT on 10000 pgd10 cifar-10 test images: 67.240000 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qlqWgSj8qOKV","executionInfo":{"status":"ok","timestamp":1619040225164,"user_tz":240,"elapsed":58604,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"b83a11ca-9ac9-4d7c-866a-6764474af44b"},"source":["# Evaluate model on pgd-20 test adv\n","correct = 0\n","total = 0\n","for i, (inputs, labels) in enumerate(cifar_adv_test20):\n","  inputs, labels = inputs.to(device), labels.to(device)\n","  with torch.no_grad():\n","    outputs = ViT_model(inputs)\n","  _, predicted = torch.max(outputs, 1)\n","  total += labels.size(0)\n","  correct += (predicted == labels).sum().item()\n","  if(i % 100 == 0):\n","    print('Accuracy on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","print('Accuracy ATTA5-ep2 ViT on %d pgd20 cifar-10 test images: %f %%' % (\n","    total,\n","    100 * correct / total))\n","result_dict[\"ViT_ATTA_5_ep2_test_pgd20_acc\"] = 100 * correct / total"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy on 64 cifar-10 test images: 73.437500 %\n","Accuracy on 6464 cifar-10 test images: 67.991955 %\n","Accuracy ATTA5-ep2 ViT on 10000 pgd20 cifar-10 test images: 67.810000 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KldxoayBsuYT"},"source":["# ATTA Training Epoch 3 ~\n"]},{"cell_type":"code","metadata":{"id":"7HRkvJFc-Myh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618975923762,"user_tz":240,"elapsed":9204553,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"8e6dd779-0ccd-4924-af52-5bf1aedc49b6"},"source":["output_dir = '../gdrive/MyDrive/EECS598-007_Group/ATTA_output/'\n","t0 = time.time()\n","for ep in range(3):\n","  print(\"##########    Epoch \" + str(ep+3) + \"    ##########\")\n","  ts = time.time()\n","  i = 0\n","  for (inputs, labels) in cifar_adv:\n","    adv_images = atk_pgd_5(inputs, labels)\n","    cifar_adv[i] = (adv_images.cpu(), labels.cpu())\n","    if i % 100 == 0:\n","      print(\"Progress(%): \", 64*i/500)\n","    i += 1\n","  time_gen_atk_2 = time.time() - ts\n","  print(\"time lap(sec):\", time_gen_atk_2)\n","  time_name = \"ViT_ATTA_5_ep\" + str(ep+3) + \"_gen_atk_sec\"\n","  result_dict[time_name] = time_gen_atk_2\n","\n","  # Store cifar-10\n","  outfile_name = 'pgd5_8p_adv_itr' + str(ep+3) + '.pt'\n","  outfile_name = output_dir + outfile_name\n","  !touch $outfile_name\n","  torch.save(cifar_adv, outfile_name)\n","  !ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/\n","\n","  ts = time.time()\n","  i = 0\n","  correct = 0\n","  total = 0\n","  running_loss = 0.0\n","  for (inputs, labels) in cifar_adv:\n","    # inputs & labels already on device\n","    # Adv training\n","    optimizer.zero_grad()\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","\n","    # forward + backward + optimize\n","    outputs = ViT_model(inputs)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    running_loss += loss.item()\n","    \n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    # print(\"Here\", np.sum(predicted == labels))\n","    if i % 150 == 0:\n","      print('Training accuracy of ViT on %d adv exs: %f %%' % (total, 100 * correct / total))\n","      print(\"running loss: \", running_loss)\n","    i += 1\n","\n","  print('Training accuracy on %d pgd-5 cifar-10 adv images: %f %%' % (\n","      total,\n","      100 * correct / total))\n","  t1 = time.time()\n","  acc_name = \"ViT_ATTA_5_ep\" + str(ep+3) + \"_train_acc\"\n","  time_name = \"ViT_ATTA_5_ep\" + str(ep+3) + \"_train_sec\"\n","  result_dict[acc_name] = 100 * correct / total\n","  result_dict[time_name] = t1 - ts\n","  print(\"Training time: \", t1 - ts)\n","\n","  # Store model after itr ep+3\n","  outfile_name = 'ATTA5_8p_adv_itr' + str(ep+3) + '_model.pt'\n","  outfile_name = output_dir + outfile_name\n","  !touch $outfile_name\n","  torch.save(ViT_model, outfile_name)\n","  !ls ../gdrive/MyDrive/EECS598-007_Group/ATTA_output/\n","\n","  #################################################################\n","  #####################    EVALUATE MODEL    ######################\n","  #################################################################\n","\n","  # Epoch 2 Model on Original test data\n","  correct = 0\n","  total = 0\n","  with torch.no_grad():\n","    for i, (inputs, labels) in enumerate(cifar_testloader):\n","      inputs, labels = inputs.to(device), labels.to(device)\n","      with torch.no_grad():\n","        outputs = ViT_model(inputs)\n","      _, predicted = torch.max(outputs, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","      if(i % 50 == 0):\n","        print('Accuracy of the network on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","  print('Accuracy of the network on %d cifar-10 test images: %f %%' % (\n","      total,\n","      100 * correct / total))\n","  orig_acc_name = \"ATTA_ep\" + str(ep+3) + \"_Orig_Acc\"\n","  result_dict[orig_acc_name] = 100 * correct / total\n","\n","  # Evaluate model on pgd-10 test adv\n","  correct = 0\n","  total = 0\n","  for i, (inputs, labels) in enumerate(cifar_adv_test10):\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    if(i % 100 == 0):\n","      print('Accuracy on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","  print('Accuracy ATTA5-ep2 ViT on %d pgd10 cifar-10 test images: %f %%' % (\n","      total,\n","      100 * correct / total))\n","  pgd10_acc_name = \"ViT_ATTA_5_ep\" + str(ep+3) + \"_test_pgd10_acc\"\n","  result_dict[pgd10_acc_name] = 100 * correct / total\n","\n","  # Evaluate model on pgd-20 test adv\n","  correct = 0\n","  total = 0\n","  for i, (inputs, labels) in enumerate(cifar_adv_test20):\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    with torch.no_grad():\n","      outputs = ViT_model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","    if(i % 100 == 0):\n","      print('Accuracy on %d cifar-10 test images: %f %%' % (total, 100 * correct / total))\n","\n","  print('Accuracy ATTA5-ep2 ViT on %d pgd20 cifar-10 test images: %f %%' % (\n","      total,\n","      100 * correct / total))\n","  pgd20_acc_name = \"ViT_ATTA_5_ep\" + str(ep+3) + \"_test_pgd20_acc\"\n","  result_dict[pgd20_acc_name] = 100 * correct / total\n","\n","t1 = time.time()\n","result_dict[\"ViT_3eps_ATTA_cifar10_sec\"] = t1 - t0"],"execution_count":25,"outputs":[{"output_type":"stream","text":["##########    Epoch 3    ##########\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Progress(%):  0.0\n","Progress(%):  12.8\n","Progress(%):  25.6\n","Progress(%):  38.4\n","Progress(%):  51.2\n","Progress(%):  64.0\n","Progress(%):  76.8\n","Progress(%):  89.6\n","time lap(sec): 2453.5920791625977\n","ATTA5_8p_adv_itr1_model.pt  pgd5_8p_adv_itr2.pt\t\tpgd_5_adv_itr3_model.pt\n","ATTA5_8p_adv_itr2_model.pt  pgd5_8p_adv_itr3.pt\t\tpgd_5_adv_itr3.pt\n","ATTA_5_adv_itr1_model.pt    pgd_5_adv_itr1.json\t\tpgd_5_adv_itr4_model.pt\n","ATTA_5_adv_itr2_model.pt    pgd_5_adv_itr1_model.pt\tpgd_5_adv_itr4.pt\n","pgd10_8p_adv_test.pt\t    pgd_5_adv_itr1.pt\t\tpgd_5_adv_itr5_model.pt\n","pgd_10_adv_test.pt\t    pgd_5_adv_itr1_v2_model.pt\tpgd_5_adv_itr5.pt\n","pgd20_8p_adv_test.pt\t    pgd_5_adv_itr1_v2.pt\tREADME.gdoc\n","pgd_20_adv_test.pt\t    pgd_5_adv_itr2_model.pt\n","pgd5_8p_adv_itr1.pt\t    pgd_5_adv_itr2.pt\n","Training accuracy of ViT on 64 adv exs: 1.562500 %\n","running loss:  8.26323127746582\n","Training accuracy of ViT on 9664 adv exs: 84.644040 %\n","running loss:  130.8826773762703\n","Training accuracy of ViT on 19264 adv exs: 88.963870 %\n","running loss:  192.23786561191082\n","Training accuracy of ViT on 28864 adv exs: 91.411447 %\n","running loss:  238.48171692341566\n","Training accuracy of ViT on 38464 adv exs: 92.840058 %\n","running loss:  276.60392688959837\n","Training accuracy of ViT on 48064 adv exs: 93.837383 %\n","running loss:  308.476131901145\n","Training accuracy on 50000 pgd-5 cifar-10 adv images: 94.000000 %\n","Training time:  660.7246961593628\n","ATTA5_8p_adv_itr1_model.pt  pgd5_8p_adv_itr1.pt\t\tpgd_5_adv_itr2.pt\n","ATTA5_8p_adv_itr2_model.pt  pgd5_8p_adv_itr2.pt\t\tpgd_5_adv_itr3_model.pt\n","ATTA5_8p_adv_itr3_model.pt  pgd5_8p_adv_itr3.pt\t\tpgd_5_adv_itr3.pt\n","ATTA_5_adv_itr1_model.pt    pgd_5_adv_itr1.json\t\tpgd_5_adv_itr4_model.pt\n","ATTA_5_adv_itr2_model.pt    pgd_5_adv_itr1_model.pt\tpgd_5_adv_itr4.pt\n","pgd10_8p_adv_test.pt\t    pgd_5_adv_itr1.pt\t\tpgd_5_adv_itr5_model.pt\n","pgd_10_adv_test.pt\t    pgd_5_adv_itr1_v2_model.pt\tpgd_5_adv_itr5.pt\n","pgd20_8p_adv_test.pt\t    pgd_5_adv_itr1_v2.pt\tREADME.gdoc\n","pgd_20_adv_test.pt\t    pgd_5_adv_itr2_model.pt\n","Accuracy of the network on 64 cifar-10 test images: 81.250000 %\n","Accuracy of the network on 3264 cifar-10 test images: 85.263480 %\n","Accuracy of the network on 6464 cifar-10 test images: 85.148515 %\n","Accuracy of the network on 9664 cifar-10 test images: 84.840646 %\n","Accuracy of the network on 10000 cifar-10 test images: 84.820000 %\n","Accuracy on 64 cifar-10 test images: 79.687500 %\n","Accuracy on 6464 cifar-10 test images: 82.564975 %\n","Accuracy ATTA5-ep2 ViT on 10000 pgd10 cifar-10 test images: 82.360000 %\n","Accuracy on 64 cifar-10 test images: 81.250000 %\n","Accuracy on 6464 cifar-10 test images: 82.827970 %\n","Accuracy ATTA5-ep2 ViT on 10000 pgd20 cifar-10 test images: 82.490000 %\n","##########    Epoch 4    ##########\n","Progress(%):  0.0\n","Progress(%):  12.8\n","Progress(%):  25.6\n","Progress(%):  38.4\n","Progress(%):  51.2\n","Progress(%):  64.0\n","Progress(%):  76.8\n","Progress(%):  89.6\n","time lap(sec): 2446.890088558197\n","ATTA5_8p_adv_itr1_model.pt  pgd5_8p_adv_itr1.pt\t\tpgd_5_adv_itr2_model.pt\n","ATTA5_8p_adv_itr2_model.pt  pgd5_8p_adv_itr2.pt\t\tpgd_5_adv_itr2.pt\n","ATTA5_8p_adv_itr3_model.pt  pgd5_8p_adv_itr3.pt\t\tpgd_5_adv_itr3_model.pt\n","ATTA_5_adv_itr1_model.pt    pgd5_8p_adv_itr4.pt\t\tpgd_5_adv_itr3.pt\n","ATTA_5_adv_itr2_model.pt    pgd_5_adv_itr1.json\t\tpgd_5_adv_itr4_model.pt\n","pgd10_8p_adv_test.pt\t    pgd_5_adv_itr1_model.pt\tpgd_5_adv_itr4.pt\n","pgd_10_adv_test.pt\t    pgd_5_adv_itr1.pt\t\tpgd_5_adv_itr5_model.pt\n","pgd20_8p_adv_test.pt\t    pgd_5_adv_itr1_v2_model.pt\tpgd_5_adv_itr5.pt\n","pgd_20_adv_test.pt\t    pgd_5_adv_itr1_v2.pt\tREADME.gdoc\n","Training accuracy of ViT on 64 adv exs: 3.125000 %\n","running loss:  8.031440734863281\n","Training accuracy of ViT on 9664 adv exs: 82.812500 %\n","running loss:  131.1608920842409\n","Training accuracy of ViT on 19264 adv exs: 89.581603 %\n","running loss:  176.46450816839933\n","Training accuracy of ViT on 28864 adv exs: 92.430017 %\n","running loss:  205.5694287456572\n","Training accuracy of ViT on 38464 adv exs: 94.012583 %\n","running loss:  228.2731946837157\n","Training accuracy of ViT on 48064 adv exs: 95.014980 %\n","running loss:  247.22980510815978\n","Training accuracy on 50000 pgd-5 cifar-10 adv images: 95.178000 %\n","Training time:  660.5210947990417\n","ATTA5_8p_adv_itr1_model.pt  pgd5_8p_adv_itr1.pt\t\tpgd_5_adv_itr2.pt\n","ATTA5_8p_adv_itr2_model.pt  pgd5_8p_adv_itr2.pt\t\tpgd_5_adv_itr3_model.pt\n","ATTA5_8p_adv_itr3_model.pt  pgd5_8p_adv_itr3.pt\t\tpgd_5_adv_itr3.pt\n","ATTA5_8p_adv_itr4_model.pt  pgd5_8p_adv_itr4.pt\t\tpgd_5_adv_itr4_model.pt\n","ATTA_5_adv_itr1_model.pt    pgd_5_adv_itr1.json\t\tpgd_5_adv_itr4.pt\n","ATTA_5_adv_itr2_model.pt    pgd_5_adv_itr1_model.pt\tpgd_5_adv_itr5_model.pt\n","pgd10_8p_adv_test.pt\t    pgd_5_adv_itr1.pt\t\tpgd_5_adv_itr5.pt\n","pgd_10_adv_test.pt\t    pgd_5_adv_itr1_v2_model.pt\tREADME.gdoc\n","pgd20_8p_adv_test.pt\t    pgd_5_adv_itr1_v2.pt\n","pgd_20_adv_test.pt\t    pgd_5_adv_itr2_model.pt\n","Accuracy of the network on 64 cifar-10 test images: 68.750000 %\n","Accuracy of the network on 3264 cifar-10 test images: 72.916667 %\n","Accuracy of the network on 6464 cifar-10 test images: 72.710396 %\n","Accuracy of the network on 9664 cifar-10 test images: 72.526904 %\n","Accuracy of the network on 10000 cifar-10 test images: 72.500000 %\n","Accuracy on 64 cifar-10 test images: 67.187500 %\n","Accuracy on 6464 cifar-10 test images: 69.879332 %\n","Accuracy ATTA5-ep2 ViT on 10000 pgd10 cifar-10 test images: 69.490000 %\n","Accuracy on 64 cifar-10 test images: 68.750000 %\n","Accuracy on 6464 cifar-10 test images: 70.095916 %\n","Accuracy ATTA5-ep2 ViT on 10000 pgd20 cifar-10 test images: 69.910000 %\n","##########    Epoch 5    ##########\n","Progress(%):  0.0\n","Progress(%):  12.8\n","Progress(%):  25.6\n","Progress(%):  38.4\n","Progress(%):  51.2\n","Progress(%):  64.0\n","Progress(%):  76.8\n","Progress(%):  89.6\n","time lap(sec): 2446.298374414444\n","ATTA5_8p_adv_itr1_model.pt  pgd5_8p_adv_itr1.pt\t\tpgd_5_adv_itr2_model.pt\n","ATTA5_8p_adv_itr2_model.pt  pgd5_8p_adv_itr2.pt\t\tpgd_5_adv_itr2.pt\n","ATTA5_8p_adv_itr3_model.pt  pgd5_8p_adv_itr3.pt\t\tpgd_5_adv_itr3_model.pt\n","ATTA5_8p_adv_itr4_model.pt  pgd5_8p_adv_itr4.pt\t\tpgd_5_adv_itr3.pt\n","ATTA_5_adv_itr1_model.pt    pgd5_8p_adv_itr5.pt\t\tpgd_5_adv_itr4_model.pt\n","ATTA_5_adv_itr2_model.pt    pgd_5_adv_itr1.json\t\tpgd_5_adv_itr4.pt\n","pgd10_8p_adv_test.pt\t    pgd_5_adv_itr1_model.pt\tpgd_5_adv_itr5_model.pt\n","pgd_10_adv_test.pt\t    pgd_5_adv_itr1.pt\t\tpgd_5_adv_itr5.pt\n","pgd20_8p_adv_test.pt\t    pgd_5_adv_itr1_v2_model.pt\tREADME.gdoc\n","pgd_20_adv_test.pt\t    pgd_5_adv_itr1_v2.pt\n","Training accuracy of ViT on 64 adv exs: 3.125000 %\n","running loss:  8.12190055847168\n","Training accuracy of ViT on 9664 adv exs: 87.500000 %\n","running loss:  115.14225921034813\n","Training accuracy of ViT on 19264 adv exs: 92.441860 %\n","running loss:  151.41396357119083\n","Training accuracy of ViT on 28864 adv exs: 94.408259 %\n","running loss:  179.4021986015141\n","Training accuracy of ViT on 38464 adv exs: 95.390495 %\n","running loss:  202.27374425902963\n","Training accuracy of ViT on 48064 adv exs: 96.113515 %\n","running loss:  220.40457546897233\n","Training accuracy on 50000 pgd-5 cifar-10 adv images: 96.218000 %\n","Training time:  660.5010859966278\n","ATTA5_8p_adv_itr1_model.pt  pgd_20_adv_test.pt\t\tpgd_5_adv_itr1_v2.pt\n","ATTA5_8p_adv_itr2_model.pt  pgd5_8p_adv_itr1.pt\t\tpgd_5_adv_itr2_model.pt\n","ATTA5_8p_adv_itr3_model.pt  pgd5_8p_adv_itr2.pt\t\tpgd_5_adv_itr2.pt\n","ATTA5_8p_adv_itr4_model.pt  pgd5_8p_adv_itr3.pt\t\tpgd_5_adv_itr3_model.pt\n","ATTA5_8p_adv_itr5_model.pt  pgd5_8p_adv_itr4.pt\t\tpgd_5_adv_itr3.pt\n","ATTA_5_adv_itr1_model.pt    pgd5_8p_adv_itr5.pt\t\tpgd_5_adv_itr4_model.pt\n","ATTA_5_adv_itr2_model.pt    pgd_5_adv_itr1.json\t\tpgd_5_adv_itr4.pt\n","pgd10_8p_adv_test.pt\t    pgd_5_adv_itr1_model.pt\tpgd_5_adv_itr5_model.pt\n","pgd_10_adv_test.pt\t    pgd_5_adv_itr1.pt\t\tpgd_5_adv_itr5.pt\n","pgd20_8p_adv_test.pt\t    pgd_5_adv_itr1_v2_model.pt\tREADME.gdoc\n","Accuracy of the network on 64 cifar-10 test images: 76.562500 %\n","Accuracy of the network on 3264 cifar-10 test images: 78.492647 %\n","Accuracy of the network on 6464 cifar-10 test images: 78.326114 %\n","Accuracy of the network on 9664 cifar-10 test images: 78.156043 %\n","Accuracy of the network on 10000 cifar-10 test images: 78.200000 %\n","Accuracy on 64 cifar-10 test images: 78.125000 %\n","Accuracy on 6464 cifar-10 test images: 76.144802 %\n","Accuracy ATTA5-ep2 ViT on 10000 pgd10 cifar-10 test images: 76.380000 %\n","Accuracy on 64 cifar-10 test images: 81.250000 %\n","Accuracy on 6464 cifar-10 test images: 76.531559 %\n","Accuracy ATTA5-ep2 ViT on 10000 pgd20 cifar-10 test images: 76.520000 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RFoBS8kMPoDU","executionInfo":{"status":"ok","timestamp":1618975967089,"user_tz":240,"elapsed":397,"user":{"displayName":"Donly For","photoUrl":"","userId":"09047299248357364289"}},"outputId":"708931bd-b17c-4734-e52a-2b6d29d6eec3"},"source":["result_dict"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ATTA_ep1_Orig_Acc': 93.21,\n"," 'ATTA_ep2_Orig_Acc': 89.22,\n"," 'ATTA_ep3_Orig_Acc': 84.82,\n"," 'ATTA_ep4_Orig_Acc': 72.5,\n"," 'ATTA_ep5_Orig_Acc': 78.2,\n"," 'ViT_3eps_ATTA_cifar10_sec': 9635.233304738998,\n"," 'ViT_ATTA_5_ep1_test_pgd10_acc': 91.29,\n"," 'ViT_ATTA_5_ep1_test_pgd20_acc': 91.07,\n"," 'ViT_ATTA_5_ep1_train_acc': 93.79,\n"," 'ViT_ATTA_5_ep1_train_sec': 663.3380136489868,\n"," 'ViT_ATTA_5_ep2_test_pgd10_acc': 81.74,\n"," 'ViT_ATTA_5_ep2_test_pgd20_acc': 82.49,\n"," 'ViT_ATTA_5_ep2_train_acc': 95.7,\n"," 'ViT_ATTA_5_ep2_train_sec': 663.5174517631531,\n"," 'ViT_ATTA_5_ep3_gen_atk_sec': 2453.5920791625977,\n"," 'ViT_ATTA_5_ep3_test_pgd10_acc': 82.36,\n"," 'ViT_ATTA_5_ep3_test_pgd20_acc': 82.49,\n"," 'ViT_ATTA_5_ep3_train_acc': 94.0,\n"," 'ViT_ATTA_5_ep3_train_sec': 660.7246961593628,\n"," 'ViT_ATTA_5_ep4_gen_atk_sec': 2446.890088558197,\n"," 'ViT_ATTA_5_ep4_test_pgd10_acc': 69.49,\n"," 'ViT_ATTA_5_ep4_test_pgd20_acc': 69.91,\n"," 'ViT_ATTA_5_ep4_train_acc': 95.178,\n"," 'ViT_ATTA_5_ep4_train_sec': 660.5210947990417,\n"," 'ViT_ATTA_5_ep5_gen_atk_sec': 2446.298374414444,\n"," 'ViT_ATTA_5_ep5_test_pgd10_acc': 76.38,\n"," 'ViT_ATTA_5_ep5_test_pgd20_acc': 76.52,\n"," 'ViT_ATTA_5_ep5_train_acc': 96.218,\n"," 'ViT_ATTA_5_ep5_train_sec': 660.5010859966278}"]},"metadata":{"tags":[]},"execution_count":26}]}]}