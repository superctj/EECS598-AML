{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PGD dataset generation.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8QOiNUnP4m-",
        "outputId": "31579366-af25-4bbb-bca2-c31a1d2cf67c"
      },
      "source": [
        "#@title Installation\n",
        "!pip install timm foolbox"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting timm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/89/d94f59780b5dd973154bf506d8ce598f6bfe7cc44dd445d644d6d3be8c39/timm-0.4.5-py3-none-any.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 5.8MB/s \n",
            "\u001b[?25hCollecting foolbox\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ff/1ba817ad9aa7c2ca28fb809d64939bee7311e3e62fdd87c4011232c4640e/foolbox-3.3.1-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.1+cu101)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.8.1+cu101)\n",
            "Collecting eagerpy==0.29.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/07/54994565da4fc5a4840d3a434fb9bf3835b4a4e68c931ccfcc327d568f95/eagerpy-0.29.0-py3-none-any.whl\n",
            "Collecting requests>=2.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from foolbox) (1.4.1)\n",
            "Collecting GitPython>=3.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 19.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from foolbox) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.7/dist-packages (from foolbox) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from foolbox) (56.0.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: timm, eagerpy, requests, smmap, gitdb, GitPython, foolbox\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed GitPython-3.1.14 eagerpy-0.29.0 foolbox-3.3.1 gitdb-4.0.7 requests-2.25.1 smmap-4.0.0 timm-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8n7mVp2P91g",
        "outputId": "186b7e42-7f73-4556-d4e2-5b20cfdc5024"
      },
      "source": [
        "#@title Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEA8DEuWQYRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "112aefe0-5441-402f-ba2f-67f9a992b3f9"
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "import eagerpy as ep\n",
        "from foolbox import PyTorchModel, accuracy, samples\n",
        "from foolbox.attacks import L2PGD\n",
        "\n",
        "import timm\n",
        "\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Version:  1.8.1+cu101\n",
            "Torchvision Version:  0.9.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFl0hz_pQpuI"
      },
      "source": [
        "#@title Global Variables\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/598dataset/tinyImageNet/TinyImageNet/\"\n",
        "generated_dir = \"/content/drive/MyDrive/598dataset/detector_pgd_2/\"\n",
        "#generated_dir = \"/content/drive/MyDrive/598dataset/detector_pgd_5/\"\n",
        "#generated_dir = \"/content/drive/MyDrive/598dataset/detector_pgd_10/\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 16\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 32\n",
        "\n",
        "input_size = 224\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bht8kKzkRAxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5383d0d3-6a2c-4249-b99d-a0ee70281938"
      },
      "source": [
        "#@title timm model creation\n",
        "model_ft = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "num_ftrs = model_ft.head.in_features\n",
        "model_ft.head = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "model_ft.load_state_dict(torch.load('/content/drive/MyDrive/598dataset/ViTbase_16_224_tiny_15classes.pth'))\n",
        "model_ft = model_ft.to(device)\n",
        "model_ft.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth\" to /root/.cache/torch/hub/checkpoints/jx_vit_base_p16_224-80ecf9dd.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  )\n",
              "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "  (blocks): ModuleList(\n",
              "    (0): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (3): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (4): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (5): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (6): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (7): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (8): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (9): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (10): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (11): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  (pre_logits): Identity()\n",
              "  (head): Linear(in_features=768, out_features=16, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9t0AWjZRVG6"
      },
      "source": [
        "#@title foolbox model def\n",
        "preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n",
        "fmodel = PyTorchModel(model_ft, bounds=(0, 1), preprocessing=preprocessing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCoZolzXSPOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f8b425-c0cf-4650-d0ef-45a1e2006441"
      },
      "source": [
        "#@title data loader init\n",
        "transform = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, 'tiny_'+x),\n",
        "                                          transform[x])\n",
        "                  for x in ['train', 'val']}\n",
        "print(len(image_datasets['train'].classes))                  \n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
        "                                             shuffle=False, num_workers=0, pin_memory=True)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n",
            "['0', '1', '10', '11', '12', '13', '14', '15', '2', '3', '4', '5', '6', '7', '8', '9']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5ybEyOSS650"
      },
      "source": [
        "#@title attack def\n",
        "\n",
        "attack = L2PGD(rel_stepsize=0.8, steps=2)\n",
        "#attack = L2PGD(rel_stepsize=0.3, steps=5)\n",
        "#attack = L2PGD(rel_stepsize=0.15, steps=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUqrKMphUmtg",
        "outputId": "b07b26e3-7cc0-4ad8-dfea-6a0691bb42d9"
      },
      "source": [
        "#@title perform attack and save train image\n",
        "epsilons = [\n",
        "    0.5,\n",
        "]\n",
        "count = 0\n",
        "running_acc = torch.zeros(len(epsilons)).to(device)\n",
        "train_storage_path = os.path.join(generated_dir,'train/adv')\n",
        "with tqdm(dataloaders['train'], unit='batch') as tepoch:\n",
        "  for inputs, labels in tepoch:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    raw_advs, clipped_advs, success = attack(fmodel, inputs, labels, epsilons=epsilons)\n",
        "    for i in clipped_advs:\n",
        "      for j in range(batch_size):\n",
        "        torchvision.utils.save_image(i[j,...], os.path.join(train_storage_path,str(count) + '.png'))\n",
        "    running_acc += 1 - success.float().mean(axis=-1)\n",
        "    count += 1\n",
        "running_acc /= count\n",
        "for eps, acc in zip(epsilons, running_acc):\n",
        "    print(f\"  L2 norm ≤ {eps:<6}: {acc.item() * 100:4.1f} %\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [1:25:38<00:00, 10.28s/batch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  L2 norm ≤ 0.5   : 36.6 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knmlr4azX-P0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3d7dc9-3f1f-4535-f796-38b381b49964"
      },
      "source": [
        "#@title perform attack and save test image\n",
        "epsilons = [\n",
        "    0.5,\n",
        "]\n",
        "count = 0\n",
        "running_acc = torch.zeros(len(epsilons)).to(device)\n",
        "train_storage_path = os.path.join(generated_dir,'val/adv')\n",
        "clipped_advs_list_val = []\n",
        "with tqdm(dataloaders['val'], unit='batch') as tepoch:\n",
        "  for inputs, labels in tepoch:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    raw_advs, clipped_advs, success = attack(fmodel, inputs, labels, epsilons=epsilons)\n",
        "    for i in clipped_advs:\n",
        "      for j in range(batch_size):\n",
        "        torchvision.utils.save_image(i[j,...], os.path.join(train_storage_path,str(count) + '.png'))\n",
        "    running_acc += 1 - success.float().mean(axis=-1)\n",
        "    count += 1\n",
        "running_acc /= count\n",
        "for eps, acc in zip(epsilons, running_acc):\n",
        "    print(f\"  L2 norm ≤ {eps:<6}: {acc.item() * 100:4.1f} %\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [10:02<00:00, 12.04s/batch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  L2 norm ≤ 0.5   : 49.8 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS8O80w8tqOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded68af9-7e01-44aa-b361-bd575140da07"
      },
      "source": [
        "#@title PGD5\n",
        "generated_dir = \"/content/drive/MyDrive/598dataset/detector_pgd_5/\"\n",
        "attack = L2PGD(rel_stepsize=0.3, steps=5)\n",
        "epsilons = [\n",
        "    0.5,\n",
        "]\n",
        "count = 0\n",
        "running_acc = torch.zeros(len(epsilons)).to(device)\n",
        "train_storage_path = os.path.join(generated_dir,'train/adv')\n",
        "with tqdm(dataloaders['train'], unit='batch') as tepoch:\n",
        "  for inputs, labels in tepoch:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    raw_advs, clipped_advs, success = attack(fmodel, inputs, labels, epsilons=epsilons)\n",
        "    for i in clipped_advs:\n",
        "      for j in range(batch_size):\n",
        "        torchvision.utils.save_image(i[j,...], os.path.join(train_storage_path,str(count) + '.png'))\n",
        "    running_acc += 1 - success.float().mean(axis=-1)\n",
        "    count += 1\n",
        "running_acc /= count\n",
        "for eps, acc in zip(epsilons, running_acc):\n",
        "    print(f\"  L2 norm ≤ {eps:<6}: {acc.item() * 100:4.1f} %\")\n",
        "\n",
        "epsilons = [\n",
        "    0.5,\n",
        "]\n",
        "count = 0\n",
        "running_acc = torch.zeros(len(epsilons)).to(device)\n",
        "train_storage_path = os.path.join(generated_dir,'val/adv')\n",
        "with tqdm(dataloaders['val'], unit='batch') as tepoch:\n",
        "  for inputs, labels in tepoch:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    raw_advs, clipped_advs, success = attack(fmodel, inputs, labels, epsilons=epsilons)\n",
        "    for i in clipped_advs:\n",
        "      for j in range(batch_size):\n",
        "        torchvision.utils.save_image(i[j,...], os.path.join(train_storage_path,str(count) + '.png'))\n",
        "    running_acc += 1 - success.float().mean(axis=-1)\n",
        "    count += 1\n",
        "running_acc /= count\n",
        "for eps, acc in zip(epsilons, running_acc):\n",
        "    print(f\"  L2 norm ≤ {eps:<6}: {acc.item() * 100:4.1f} %\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [33:31<00:00,  4.02s/batch]\n",
            "  0%|          | 0/50 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  L2 norm ≤ 0.5   : 28.6 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [03:20<00:00,  4.00s/batch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  L2 norm ≤ 0.5   : 40.6 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sibxFAhAJgV3",
        "outputId": "b6481340-9626-4961-b14c-b3fb8ad4563e"
      },
      "source": [
        "#@title PGD10\n",
        "generated_dir = \"/content/drive/MyDrive/598dataset/detector_pgd_10/\"\n",
        "attack = L2PGD(rel_stepsize=0.15, steps=10)\n",
        "epsilons = [\n",
        "    0.5,\n",
        "]\n",
        "count = 0\n",
        "running_acc = torch.zeros(len(epsilons)).to(device)\n",
        "train_storage_path = os.path.join(generated_dir,'train/adv')\n",
        "with tqdm(dataloaders['train'], unit='batch') as tepoch:\n",
        "  for inputs, labels in tepoch:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    raw_advs, clipped_advs, success = attack(fmodel, inputs, labels, epsilons=epsilons)\n",
        "    for i in clipped_advs:\n",
        "      for j in range(batch_size):\n",
        "        torchvision.utils.save_image(i[j,...], os.path.join(train_storage_path,str(count) + '.png'))\n",
        "    running_acc += 1 - success.float().mean(axis=-1)\n",
        "    count += 1\n",
        "running_acc /= count\n",
        "for eps, acc in zip(epsilons, running_acc):\n",
        "    print(f\"  L2 norm ≤ {eps:<6}: {acc.item() * 100:4.1f} %\")\n",
        "\n",
        "epsilons = [\n",
        "    0.5,\n",
        "]\n",
        "count = 0\n",
        "running_acc = torch.zeros(len(epsilons)).to(device)\n",
        "train_storage_path = os.path.join(generated_dir,'val/adv')\n",
        "with tqdm(dataloaders['val'], unit='batch') as tepoch:\n",
        "  for inputs, labels in tepoch:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    raw_advs, clipped_advs, success = attack(fmodel, inputs, labels, epsilons=epsilons)\n",
        "    for i in clipped_advs:\n",
        "      for j in range(batch_size):\n",
        "        torchvision.utils.save_image(i[j,...], os.path.join(train_storage_path,str(count) + '.png'))\n",
        "    running_acc += 1 - success.float().mean(axis=-1)\n",
        "    count += 1\n",
        "running_acc /= count\n",
        "for eps, acc in zip(epsilons, running_acc):\n",
        "    print(f\"  L2 norm ≤ {eps:<6}: {acc.item() * 100:4.1f} %\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [2:52:00<00:00, 20.64s/batch]\n",
            "  0%|          | 0/50 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  L2 norm ≤ 0.5   : 25.6 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [16:54<00:00, 20.29s/batch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  L2 norm ≤ 0.5   : 36.7 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI1TyX2s4ZKD",
        "outputId": "3e5cb14c-a525-4086-bede-fc33213cc894"
      },
      "source": [
        "#@title pgd 1 L inf ep=0.01\n",
        "from foolbox.attacks import LinfPGD\n",
        "\n",
        "generated_dir = \"/content/drive/MyDrive/598dataset/detector_pgd_1_inf_0.01/\"\n",
        "attack = LinfPGD(rel_stepsize=1.5, steps=1)\n",
        "epsilons = [\n",
        "    0.01,\n",
        "]\n",
        "count = 0\n",
        "running_acc = torch.zeros(len(epsilons)).to(device)\n",
        "train_storage_path = os.path.join(generated_dir,'train/adv')\n",
        "with tqdm(dataloaders['train'], unit='batch') as tepoch:\n",
        "  for inputs, labels in tepoch:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    raw_advs, clipped_advs, success = attack(fmodel, inputs, labels, epsilons=epsilons)\n",
        "    for i in clipped_advs:\n",
        "      for j in range(batch_size):\n",
        "        torchvision.utils.save_image(i[j,...], os.path.join(train_storage_path,str(count) + '.png'))\n",
        "    running_acc += 1 - success.float().mean(axis=-1)\n",
        "    count += 1\n",
        "running_acc /= count\n",
        "for eps, acc in zip(epsilons, running_acc):\n",
        "    print(f\"  L2 norm ≤ {eps:<6}: {acc.item() * 100:4.1f} %\")\n",
        "\n",
        "epsilons = [\n",
        "    0.01,\n",
        "]\n",
        "count = 0\n",
        "running_acc = torch.zeros(len(epsilons)).to(device)\n",
        "train_storage_path = os.path.join(generated_dir,'val/adv')\n",
        "with tqdm(dataloaders['val'], unit='batch') as tepoch:\n",
        "  for inputs, labels in tepoch:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    raw_advs, clipped_advs, success = attack(fmodel, inputs, labels, epsilons=epsilons)\n",
        "    for i in clipped_advs:\n",
        "      for j in range(batch_size):\n",
        "        torchvision.utils.save_image(i[j,...], os.path.join(train_storage_path,str(count) + '.png'))\n",
        "    running_acc += 1 - success.float().mean(axis=-1)\n",
        "    count += 1\n",
        "running_acc /= count\n",
        "for eps, acc in zip(epsilons, running_acc):\n",
        "    print(f\"  L2 norm ≤ {eps:<6}: {acc.item() * 100:4.1f} %\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [11:44<00:00,  1.41s/batch]\n",
            "  0%|          | 0/50 [00:00<?, ?batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  L2 norm ≤ 0.01  : 21.7 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:06<00:00,  1.33s/batch]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  L2 norm ≤ 0.01  : 29.0 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}