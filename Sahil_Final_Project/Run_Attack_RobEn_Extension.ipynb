{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Run_Attack_RobEn_Extension.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDHmPLpx0mLL"
      },
      "source": [
        "# Attack RobEn Extension\n",
        "\n",
        "Here we load in a model trained using clustering similar to RobEn to be robust against synonym based substitution attacks. We then perform attacks to test the performance of the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmDZCJfQ0mLS"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QData/TextAttack/blob/master/docs/2notebook/Example_5_Explain_BERT.ipynb)\n",
        "\n",
        "[![View Source on GitHub](https://img.shields.io/badge/github-view%20source-black.svg)](https://github.com/QData/TextAttack/blob/master/docs/2notebook/Example_5_Explain_BERT.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeAY8BUz0mLT"
      },
      "source": [
        "## Pip installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-08T15:36:22.177220Z",
          "start_time": "2021-04-08T15:36:16.761401Z"
        },
        "id": "32a9uN340mLT"
      },
      "source": [
        "!pip install captum\n",
        "!pip install textattack\n",
        "!pip install torchfile\n",
        "!pip install allennlp\n",
        "!pip install pytorch_transformers \n",
        "!pip install PreTrainedTokenizer\n",
        "!pip install --upgrade flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggdoI3dPN7pV"
      },
      "source": [
        "from captum.attr import visualization as viz\n",
        "from textattack.datasets import HuggingFaceDataset\n",
        "from textattack.models.tokenizers import AutoTokenizer\n",
        "from textattack.models.wrappers import ModelWrapper, HuggingFaceModelWrapper\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from IPython.display import display, HTML\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from datasets import load_dataset\n",
        "'''\n",
        "from utils_glue import InputExample\n",
        "from utils import Clustering\n",
        "from recoverer import ClusterRepRecoverer\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_h6sH_00mLU"
      },
      "source": [
        "## Make GPU available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-08T15:43:08.573471Z",
          "start_time": "2021-04-08T15:43:08.569919Z"
        },
        "id": "8LYvEDnj0mLU"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.set_device(device)\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bfrJ9aF0mLV"
      },
      "source": [
        "## Load our model and dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-08T15:36:37.131277Z",
          "start_time": "2021-04-08T15:36:22.620178Z"
        },
        "id": "MTjJ_Z8-0mLV"
      },
      "source": [
        "dataset = load_dataset('glue', 'mrpc', split='validation')\n",
        "dir = 'MRPC_3_No_Stop'\n",
        "clustering = Clustering.from_pickle(dir+'/vocab100000_ed1.pkl', max_num_possibilities=10)\n",
        "recoverer = ClusterRepRecoverer(dir, clustering)\n",
        "original_model = BertForSequenceClassification.from_pretrained(dir)\n",
        "original_tokenizer = BertTokenizer.from_pretrained(dir)\n",
        "model_wrapper = HuggingFaceModelWrapper(original_model,original_tokenizer)\n",
        "model_wrapper.model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjMGrL9N4V7e"
      },
      "source": [
        "new_dataset = []\n",
        "for x in dataset:\n",
        "  o = OrderedDict([(\"s1\", x['sentence1']), (\"s2\", x['sentence2'])])\n",
        "  new_dataset.append([x['sentence1'],x['label']])\n",
        "print(new_dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV1UUozQ0mLW"
      },
      "source": [
        "## Define some useful functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-08T15:37:26.027504Z",
          "start_time": "2021-04-08T15:37:26.023333Z"
        },
        "id": "vTr5PIs70mLW"
      },
      "source": [
        "def captum_form(encoded):\n",
        "    input_dict = {k: [_dict[k] for _dict in encoded] for k in encoded[0]}\n",
        "    batch_encoded = {k: torch.tensor(v).to(device) for k, v in input_dict.items()}\n",
        "    return batch_encoded\n",
        "\n",
        "\n",
        "def calculate(input_ids, token_type_ids=None, position_ids=None, attention_mask=None):\n",
        "    return model_wrapper.model(\n",
        "        input_ids,\n",
        "        token_type_ids=token_type_ids,\n",
        "        position_ids=position_ids,\n",
        "        attention_mask=attention_mask,\n",
        "    ).logits\n",
        "\n",
        "\n",
        "def display_html(html_str):\n",
        "    display(HTML(html_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lGeHLn60mLX"
      },
      "source": [
        "## Pick an Attribution Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-08T15:37:33.748000Z",
          "start_time": "2021-04-08T15:37:33.745634Z"
        },
        "id": "LqrnyBwx0mLX"
      },
      "source": [
        "from captum.attr import LayerIntegratedGradients\n",
        "\n",
        "# more algorithms are avaliable at:\n",
        "# https://github.com/pytorch/captum/blob/master/docs/algorithms_comparison_matrix.md\n",
        "\n",
        "lig = LayerIntegratedGradients(calculate, model_wrapper.model.bert.embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmxlkh1z0mLX"
      },
      "source": [
        "## Pick an Attack Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-08T15:37:42.577782Z",
          "start_time": "2021-04-08T15:37:41.642168Z"
        },
        "id": "5OMMs20v0mLX"
      },
      "source": [
        "from textattack.attack_recipes import TextFoolerJin2019\n",
        "attack = TextFoolerJin2019.build(model_wrapper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-08T15:38:19.362424Z",
          "start_time": "2021-04-08T15:38:14.039570Z"
        },
        "id": "mvkO6KcL0mLY"
      },
      "source": [
        "from textattack.attack_results import FailedAttackResult\n",
        "results_iterable = attack.attack_dataset(new_dataset)\n",
        "\n",
        "viz_list = []\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for n, result in enumerate(results_iterable):\n",
        "    s1 = result.original_text().split('\\n')[0][4:]\n",
        "    s2 = result.original_text().split('\\n')[1][4:]\n",
        "    ex = recoverer.recover_example(InputExample(guid=n,text_a=s1,text_b=s2,label=new_dataset[n][1]))\n",
        "    s1_pert = result.perturbed_text().split('\\n')[0][4:]\n",
        "    s2_pert = result.perturbed_text().split('\\n')[1][4:]\n",
        "    ex_pert = recoverer.recover_example(InputExample(guid=n,text_a=s1_pert,text_b=s2_pert,label=new_dataset[n][1]))\n",
        "\n",
        "    orig = \"S1: \" + ex.text_a +\"\\n\" + \"S2: \" + ex.text_b\n",
        "    pert = \"S1: \" + ex_pert.text_a +\"\\n\" + \"S2: \" + ex_pert.text_b\n",
        "    #orig = result.original_text()\n",
        "    print('Original Sentences: ', orig)\n",
        "    #pert = result.perturbed_text()\n",
        "    print('Perturbed Sentences: ', pert)\n",
        "\n",
        "    # get prediction\n",
        "    encoded = model_wrapper.tokenizer.batch_encode([orig])\n",
        "    batch_encoded = captum_form(encoded)\n",
        "    logit = calculate(**batch_encoded).detach().cpu().numpy()\n",
        "\n",
        "    pert_encoded = model_wrapper.tokenizer.batch_encode([pert])\n",
        "    pert_batch_encoded = captum_form(pert_encoded)\n",
        "    logit_pert = calculate(**pert_batch_encoded).detach().cpu().numpy()\n",
        "\n",
        "    print(np.argmax(logit))\n",
        "    print(np.argmax(logit_pert))\n",
        "    print('Original label: ', new_dataset[n][1])\n",
        "    if (np.argmax(logit_pert) == new_dataset[n][1]):\n",
        "      correct += 1\n",
        "    total += 1\n",
        "print('Number Correct: ', correct)\n",
        "print('Total: ', total)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}